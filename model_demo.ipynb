{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Demo: MassFormer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: dgl in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (1.1.2.post1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgl) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgl) (1.11.3)\n",
      "Requirement already satisfied: networkx>=2.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgl) (3.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgl) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgl) (4.66.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgl) (5.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests>=2.19.0->dgl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests>=2.19.0->dgl) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: dgllife in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (0.3.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (1.3.2)\n",
      "Requirement already satisfied: pandas in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (2.1.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (1.11.3)\n",
      "Requirement already satisfied: networkx>=2.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (3.2.1)\n",
      "Requirement already satisfied: hyperopt in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (0.2.7)\n",
      "Requirement already satisfied: joblib in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (1.3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests>=2.22.0->dgllife) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests>=2.22.0->dgllife) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests>=2.22.0->dgllife) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests>=2.22.0->dgllife) (2023.7.22)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from scikit-learn>=0.22.2->dgllife) (3.2.0)\n",
      "Requirement already satisfied: six in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from hyperopt->dgllife) (1.16.0)\n",
      "Requirement already satisfied: future in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from hyperopt->dgllife) (0.18.3)\n",
      "Requirement already satisfied: cloudpickle in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from hyperopt->dgllife) (3.0.0)\n",
      "Requirement already satisfied: py4j in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from hyperopt->dgllife) (0.10.9.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from pandas->dgllife) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from pandas->dgllife) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from pandas->dgllife) (2023.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install dgl\n",
    "%pip install dgllife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dgl\n",
    "import dgllife.utils as chemutils\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.utils.data as th_data\n",
    "import massformer.algos1\n",
    "import massformer.algos2\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data\n",
    "from transformers.models.graphormer.collating_graphormer import GraphormerDataCollator, preprocess_item\n",
    "import src.massformer.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec_df = pd.read_pickle(\"data/proc_demo/spec_df.pkl\")\n",
    "# mol_df = pd.read_pickle(\"data/proc_demo/mol_df.pkl\")\n",
    "# toy_spec_df = spec_df.sample(n=1000, replace=False, random_state=420)\n",
    "# toy_mol_df = mol_df[mol_df[\"mol_id\"].isin(toy_spec_df[\"mol_id\"])]\n",
    "# toy_spec_df.to_pickle(\"data/proc_demo/toy_spec_df.pkl\")\n",
    "# toy_mol_df.to_pickle(\"data/proc_demo/toy_mol_df.pkl\")\n",
    "# toy_mol_df = toy_mol_df.set_index(\n",
    "#             \"mol_id\", drop=False).sort_index().rename_axis(None)\n",
    "# print(toy_spec_df)\n",
    "# print(toy_mol_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(toy_spec_df)\n",
    "# print(toy_mol_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "class MassFormerDataCollator(GraphormerDataCollator):\n",
    "\n",
    "    def __init__(self, spatial_pos_max=1024):\n",
    "\n",
    "        super().__init__(spatial_pos_max=spatial_pos_max, on_the_fly_processing=False)\n",
    "        # custom init stuff for MassFormer\n",
    "\n",
    "    def __call__(self, items: List[dict]):\n",
    "\n",
    "        print(f\"all keys = {list(items[0].keys())}\")\n",
    "\n",
    "        # this list of arguments is basically what preprocess_item produces\n",
    "        gf_keys = ['input_nodes', 'attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'out_degree', 'input_edges', 'labels']\n",
    "        # gf_related_keys = ['spatial_pos', 'labels', 'y', 'edge_index', 'edge_attr', 'num_nodes', 'attn_bias', 'input_nodes', 'attn_edge_type', 'in_degree', 'input_edges', 'out_degree', 'x']\n",
    "        gf_related_keys = ['y', 'edge_index', 'edge_attr', 'num_nodes', 'x']\n",
    "    \n",
    "        gf_items = []\n",
    "        for item in items:\n",
    "            gf_item = {}\n",
    "            for k in gf_keys:\n",
    "                # note: some keys are optional, so we need to check for them\n",
    "                if k in item:\n",
    "                    gf_item[k] = item.pop(k)\n",
    "                else:\n",
    "                    # helpful debugging message\n",
    "                    print(f\"Warning: {k} not found in item\")\n",
    "            gf_items.append(gf_item)\n",
    "\n",
    "\n",
    "        # custom call stuff for MassFormer\n",
    "        gf_collated = super().__call__(gf_items)\n",
    "        print(f\"gf_collated_keys = {list(gf_collated.keys())}\")\n",
    "\n",
    "        # After collating gf aspects, remove gf-related keys for mass spec collation\n",
    "        for item in items:\n",
    "            for k in gf_related_keys:\n",
    "                # note: some keys are optional, so we need to check for them\n",
    "                if k in item:\n",
    "                    item.pop(k)\n",
    "                else:\n",
    "                    print(f\"Warning: {k} not found in item\")\n",
    "\n",
    "        # now, let's handle the rest of the stuff\n",
    "        other_collated = {k: [] for k in items[0].keys()}\n",
    "        for data_d in items:\n",
    "            for k, v in data_d.items():\n",
    "                    other_collated[k].append(v)\n",
    "        for k, v in other_collated.items():\n",
    "            if isinstance(items[0][k], th.Tensor):\n",
    "                other_collated[k] = th.cat(v, dim=0)\n",
    "            elif isinstance(items[0][k], list):\n",
    "                other_collated[k] = utils.flatten_lol(v)\n",
    "            else:\n",
    "                raise ValueError(f\"{type(items[0][k])} is not supported\")\n",
    "\n",
    "        print(f\"other_collated_keys = {list(other_collated.keys())}\")\n",
    "\n",
    "        # now, let's combine the two collated dicts\n",
    "        both_collated = {**gf_collated, **other_collated}\n",
    "\n",
    "        return both_collated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainSubset(th_data.Subset):\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset.__getitem__(self.indices[idx])\n",
    "    \n",
    "class BaseDataset(th_data.Dataset):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "        assert os.path.isdir(self.proc_dp), self.proc_dp\n",
    "        self.spec_df = pd.read_pickle(\n",
    "            os.path.join(self.proc_dp, \"spec_df.pkl\"))\n",
    "        self.mol_df = pd.read_pickle(\n",
    "            os.path.join(self.proc_dp, \"mol_df.pkl\"))\n",
    "        self.mol_df = self.mol_df[self.mol_df[\"mol_id\"].isin(self.spec_df[\"mol_id\"])]\n",
    "\n",
    "        # generate toy spec and mol df for faster processing\n",
    "        self.toy_spec_df = self.spec_df.sample(n=2000, replace=False, random_state=420)\n",
    "        self.toy_mol_df = self.mol_df[self.mol_df[\"mol_id\"].isin(self.toy_spec_df[\"mol_id\"])]\n",
    "\n",
    "        # save toy spec and mol df as pickle\n",
    "        self.toy_spec_df.to_pickle(\"data/proc_demo/toy_spec_df.pkl\")\n",
    "        self.toy_mol_df.to_pickle(\"data/proc_demo/toy_mol_df.pkl\")\n",
    "\n",
    "        # use toy df\n",
    "        self.spec_df = self.toy_spec_df\n",
    "        self.mol_df = self.toy_mol_df\n",
    "\n",
    "\n",
    "        self._select_spec()\n",
    "        self._setup_spec_metadata_dicts()\n",
    "        # use mol_id as index for speedy access\n",
    "        self.mol_df = self.mol_df.set_index(\n",
    "            \"mol_id\", drop=False).sort_index().rename_axis(None)\n",
    "\n",
    "\n",
    "    def _select_spec(self):\n",
    "\n",
    "        masks = []\n",
    "        # dataset mask\n",
    "        dset_mask = self.spec_df[\"dset\"].isin(\n",
    "            self.primary_dset + self.secondary_dset)\n",
    "        masks.append(dset_mask)\n",
    "        # instrument type\n",
    "        inst_type_mask = self.spec_df[\"inst_type\"].isin(self.inst_type)\n",
    "        masks.append(inst_type_mask)\n",
    "        # frag mode\n",
    "        frag_mode_mask = self.spec_df[\"frag_mode\"].isin(self.frag_mode)\n",
    "        masks.append(frag_mode_mask)\n",
    "        # ion mode\n",
    "        ion_mode_mask = self.spec_df[\"ion_mode\"] == self.ion_mode\n",
    "        masks.append(ion_mode_mask)\n",
    "        # precursor type\n",
    "        if self.ion_mode == \"P\":\n",
    "            prec_type_mask = self.spec_df[\"prec_type\"].isin(self.pos_prec_type)\n",
    "        elif self.ion_mode == \"N\":\n",
    "            prec_type_mask = self.spec_df[\"prec_type\"].isin(self.neg_prec_type)\n",
    "        else:\n",
    "            assert self.ion_mode == \"EI\"\n",
    "            prec_type_mask = self.spec_df[\"prec_type\"] == \"EI\"\n",
    "        masks.append(prec_type_mask)\n",
    "        # resolution\n",
    "        if self.res != []:\n",
    "            res_mask = self.spec_df[\"res\"].isin(self.res)\n",
    "            masks.append(res_mask)\n",
    "        # collision energy\n",
    "        ce_mask = ~(self.spec_df[\"ace\"].isna() & self.spec_df[\"nce\"].isna())\n",
    "        masks.append(ce_mask)\n",
    "        # spectrum type\n",
    "        if self.ion_mode == \"EI\":\n",
    "            spec_type_mask = self.spec_df[\"spec_type\"] == \"EI\"\n",
    "        else:\n",
    "            spec_type_mask = self.spec_df[\"spec_type\"] == \"MS2\"\n",
    "        masks.append(spec_type_mask)\n",
    "        # maximum mz allowed\n",
    "        mz_mask = self.spec_df[\"peaks\"].apply(\n",
    "            lambda peaks: max(peak[0] for peak in peaks) < self.mz_max)\n",
    "        masks.append(mz_mask)\n",
    "        # precursor mz\n",
    "        prec_mz_mask = ~self.spec_df[\"prec_mz\"].isna()\n",
    "        masks.append(prec_mz_mask)\n",
    "        # single molecule\n",
    "        multi_mol_ids = self.mol_df[self.mol_df[\"smiles\"].str.contains(\n",
    "            \"\\\\.\")][\"mol_id\"]\n",
    "        single_mol_mask = ~self.spec_df[\"mol_id\"].isin(multi_mol_ids)\n",
    "        masks.append(single_mol_mask)\n",
    "        # neutral molecule\n",
    "        charges = self.mol_df[\"mol\"].apply(utils.mol_to_charge)\n",
    "        charged_ids = self.mol_df[charges != 0][\"mol_id\"]\n",
    "        neutral_mask = ~self.spec_df[\"mol_id\"].isin(charged_ids)\n",
    "        # print(neutral_mask.sum())\n",
    "        masks.append(neutral_mask)\n",
    "        # put them together\n",
    "        all_mask = masks[0]\n",
    "        for mask in masks:\n",
    "            all_mask = all_mask & mask\n",
    "        if np.sum(all_mask) == 0:\n",
    "            raise ValueError(\"select removed all items\")\n",
    "        self.spec_df = self.spec_df[all_mask].reset_index(drop=True)\n",
    "        self._setup_ce()\n",
    "        # get group_id\n",
    "        n_before_group = self.spec_df.shape[0]\n",
    "        group_df = self.spec_df.drop(\n",
    "            columns=[\n",
    "                \"spec_id\",\n",
    "                \"peaks\",\n",
    "                \"nce\",\n",
    "                \"ace\",\n",
    "                \"res\",\n",
    "                \"prec_mz\",\n",
    "                \"ri\",\n",
    "                \"col_gas\"])\n",
    "        assert not group_df.isna().any().any()\n",
    "        group_df = group_df.drop_duplicates()\n",
    "        group_df.loc[:, \"group_id\"] = np.arange(group_df.shape[0])\n",
    "        self.spec_df = self.spec_df.merge(group_df, how=\"inner\")\n",
    "        del group_df\n",
    "        n_after_group = self.spec_df.shape[0]\n",
    "        assert n_before_group == n_after_group\n",
    "        # compute stats\n",
    "        for dset in self.primary_dset + self.secondary_dset:\n",
    "            dset_spec_df = self.spec_df[self.spec_df[\"dset\"] == dset]\n",
    "            num_specs = dset_spec_df[\"spec_id\"].nunique()\n",
    "            num_mols = dset_spec_df[\"mol_id\"].nunique()\n",
    "            num_groups = dset_spec_df[\"group_id\"].nunique()\n",
    "            num_ces_per_group = dset_spec_df[[\"group_id\", self.ce_key]].groupby(\n",
    "                by=\"group_id\").count()[self.ce_key].mean()\n",
    "            print(f\">>> {dset}\")\n",
    "            print(\n",
    "                f\"> num_spec = {num_specs}, num_mol = {num_mols}, num_group = {num_groups}, num_ce_per_group = {num_ces_per_group}\")\n",
    "        # subsample\n",
    "        if self.subsample_size > 0:\n",
    "            self.spec_df = self.spec_df.groupby(\"mol_id\").sample(\n",
    "                n=self.subsample_size, random_state=self.subsample_seed, replace=True)\n",
    "            self.spec_df = self.spec_df.reset_index(drop=True)\n",
    "        else:\n",
    "            self.spec_df = self.spec_df\n",
    "        # num_entries\n",
    "        if self.num_entries > 0:\n",
    "            self.spec_df = self.spec_df.sample(\n",
    "                n=self.num_entries,\n",
    "                random_state=self.subsample_seed,\n",
    "                replace=False)\n",
    "            self.spec_df = self.spec_df.reset_index(drop=True)\n",
    "        # only keep mols with spectra\n",
    "        self.mol_df = self.mol_df[self.mol_df[\"mol_id\"].isin(\n",
    "            self.spec_df[\"mol_id\"])]\n",
    "        self.mol_df = self.mol_df.reset_index(drop=True)\n",
    "\n",
    "    def _setup_ce(self):\n",
    "\n",
    "        if self.convert_ce:\n",
    "            if self.ce_key == \"ace\":\n",
    "                other_ce_key = \"nce\"\n",
    "                ce_conversion_fn = utils.nce_to_ace\n",
    "            else:\n",
    "                other_ce_key = \"ace\"\n",
    "                ce_conversion_fn = utils.ace_to_nce\n",
    "            convert_mask = self.spec_df[self.ce_key].isna()\n",
    "            assert not self.spec_df.loc[convert_mask,\n",
    "                                        other_ce_key].isna().any()\n",
    "            self.spec_df.loc[convert_mask, self.ce_key] = self.spec_df[convert_mask].apply(\n",
    "                ce_conversion_fn, axis=1)\n",
    "            assert not self.spec_df[self.ce_key].isna().any()\n",
    "        else:\n",
    "            self.spec_df = self.spec_df.dropna(\n",
    "                axis=0, subset=[\n",
    "                    self.ce_key]).reset_index(\n",
    "                drop=True)\n",
    "\n",
    "    def _setup_spec_metadata_dicts(self):\n",
    "\n",
    "        # featurize spectral metadata\n",
    "        # we can assume that the dataset is filtered (using the method above)\n",
    "        # to only include these values\n",
    "        inst_type_list = self.inst_type\n",
    "        if self.ion_mode == \"P\":\n",
    "            prec_type_list = self.pos_prec_type\n",
    "        elif self.ion_mode == \"N\":\n",
    "            prec_type_list = self.neg_prec_type\n",
    "        else:\n",
    "            assert self.ion_mode == \"EI\"\n",
    "            prec_type_list = [\"EI\"]\n",
    "        frag_mode_list = self.frag_mode\n",
    "        self.inst_type_c2i = {\n",
    "            string: i for i,\n",
    "            string in enumerate(inst_type_list)}\n",
    "        self.inst_type_i2c = {\n",
    "            i: string for i,\n",
    "            string in enumerate(inst_type_list)}\n",
    "        self.prec_type_c2i = {\n",
    "            string: i for i,\n",
    "            string in enumerate(prec_type_list)}\n",
    "        self.prec_type_i2c = {\n",
    "            i: string for i,\n",
    "            string in enumerate(prec_type_list)}\n",
    "        self.frag_mode_c2i = {\n",
    "            string: i for i,\n",
    "            string in enumerate(frag_mode_list)}\n",
    "        self.frag_mode_i2c = {\n",
    "            i: string for i,\n",
    "            string in enumerate(frag_mode_list)}\n",
    "        self.num_inst_type = len(inst_type_list)\n",
    "        self.num_prec_type = len(prec_type_list)\n",
    "        self.num_frag_mode = len(frag_mode_list)\n",
    "        self.max_ce = self.spec_df[self.ce_key].max()\n",
    "        self.mean_ce = self.spec_df[self.ce_key].mean()\n",
    "        self.std_ce = self.spec_df[self.ce_key].std()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        spec_entry = self.spec_df.iloc[idx]\n",
    "        mol_id = spec_entry[\"mol_id\"]\n",
    "        # mol_entry = self.mol_df[self.mol_df[\"mol_id\"] == mol_id].iloc[0]\n",
    "        mol_entry = self.mol_df.loc[mol_id]\n",
    "        data = self.process_entry(spec_entry, mol_entry[\"mol\"])\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.spec_df.shape[0]\n",
    "\n",
    "    def bin_func(self, mzs, ints, return_index=False):\n",
    "\n",
    "        assert self.ints_thresh == 0., self.ints_thresh\n",
    "        return utils.bin_func(\n",
    "            mzs,\n",
    "            ints,\n",
    "            self.mz_max,\n",
    "            self.mz_bin_res,\n",
    "            self.ints_thresh,\n",
    "            return_index)\n",
    "\n",
    "    def transform_func(self, spec):\n",
    "\n",
    "        if self.process_spec_old:\n",
    "            spec = utils.process_spec_old(\n",
    "                spec,\n",
    "                self.transform,\n",
    "                self.spectrum_normalization,\n",
    "                self.ints_thresh)\n",
    "        else:\n",
    "            spec = utils.process_spec(\n",
    "                th.as_tensor(spec),\n",
    "                self.transform,\n",
    "                self.spectrum_normalization)\n",
    "            spec = spec.numpy()\n",
    "        return spec\n",
    "\n",
    "    def get_split_masks(\n",
    "            self,\n",
    "            val_frac,\n",
    "            test_frac,\n",
    "            sec_frac,\n",
    "            split_key,\n",
    "            split_seed):\n",
    "\n",
    "        assert split_key in [\"inchikey_s\", \"scaffold\"], split_key\n",
    "        assert len(self.secondary_dset) <= 1, self.secondary_dset\n",
    "        # primary\n",
    "        prim_mask = self.spec_df[\"dset\"].isin(self.primary_dset)\n",
    "        prim_mol_id = self.spec_df[prim_mask][\"mol_id\"].unique()\n",
    "        prim_key = set(\n",
    "            self.mol_df[self.mol_df[\"mol_id\"].isin(prim_mol_id)][split_key])\n",
    "        # secondary\n",
    "        sec_mask = self.spec_df[\"dset\"].isin(self.secondary_dset)\n",
    "        sec_mol_id = self.spec_df[sec_mask][\"mol_id\"].unique()\n",
    "        sec_key = set(\n",
    "            self.mol_df[self.mol_df[\"mol_id\"].isin(sec_mol_id)][split_key])\n",
    "        sec_key_list = sorted(list(sec_key))\n",
    "        # print(sec_key_list[:5])\n",
    "        # sample secondary keys\n",
    "        with utils.np_temp_seed(split_seed):\n",
    "            sec_num = round(len(sec_key_list) * sec_frac)\n",
    "            sec_key_list = np.random.choice(\n",
    "                sec_key_list, size=sec_num, replace=False).tolist()\n",
    "            sec_key = set(sec_key_list)\n",
    "            # print(sec_key_list[:5])\n",
    "            sec_mol_id = self.mol_df[self.mol_df[split_key].isin(\n",
    "                sec_key_list) & self.mol_df[\"mol_id\"].isin(sec_mol_id)][\"mol_id\"].unique()\n",
    "            sec_mask = self.spec_df[\"mol_id\"].isin(sec_mol_id) & sec_mask\n",
    "            # print(split_seed,sec_num,sec_mask.sum())\n",
    "        # get keys (secondary might same compounds as primary does!)\n",
    "        prim_only_key = prim_key - sec_key\n",
    "        sec_only_key = sec_key\n",
    "        prim_only_key_list = sorted(list(prim_only_key))\n",
    "        both_key = prim_key & sec_key\n",
    "        # compute number for each split\n",
    "        test_num = round(len(prim_only_key_list) * test_frac)\n",
    "        val_num = round(len(prim_only_key_list) * val_frac)\n",
    "        # make sure that test set gets all of the casmi keys!\n",
    "        prim_only_and_casmi = set()\n",
    "        if test_num > 0:\n",
    "            assert len(prim_only_and_casmi) <= test_num\n",
    "            test_num -= len(prim_only_and_casmi)\n",
    "        prim_only_no_casmi_key_list = [\n",
    "            k for k in prim_only_key_list if not (\n",
    "                k in prim_only_and_casmi)]\n",
    "        assert len(set(prim_only_no_casmi_key_list) & prim_only_and_casmi) == 0\n",
    "        # do the split\n",
    "        with utils.np_temp_seed(split_seed):\n",
    "            prim_only_test_num = max(test_num - len(both_key), 0)\n",
    "            test_key = set(\n",
    "                np.random.choice(\n",
    "                    prim_only_no_casmi_key_list,\n",
    "                    size=prim_only_test_num,\n",
    "                    replace=False))\n",
    "            test_key = test_key.union(prim_only_and_casmi).union(both_key)\n",
    "            train_val_key = prim_only_key - test_key\n",
    "            val_key = set(\n",
    "                np.random.choice(\n",
    "                    sorted(\n",
    "                        list(train_val_key)),\n",
    "                    size=val_num,\n",
    "                    replace=False))\n",
    "            train_key = train_val_key - val_key\n",
    "            assert len(train_key & sec_only_key) == 0\n",
    "            assert len(val_key & sec_only_key) == 0\n",
    "            # assert len(test_key & sec_only_key) == 0\n",
    "            assert len(train_key & prim_only_and_casmi) == 0\n",
    "            assert len(val_key & prim_only_and_casmi) == 0\n",
    "        # get ids and create masks\n",
    "        train_mol_id = self.mol_df[\"mol_id\"][self.mol_df[split_key].isin(\n",
    "            list(train_key))].unique()\n",
    "        val_mol_id = self.mol_df[\"mol_id\"][self.mol_df[split_key].isin(\n",
    "            list(val_key))].unique()\n",
    "        test_mol_id = self.mol_df[\"mol_id\"][self.mol_df[split_key].isin(\n",
    "            list(test_key))].unique()\n",
    "        train_mask = self.spec_df[\"mol_id\"].isin(train_mol_id)\n",
    "        val_mask = self.spec_df[\"mol_id\"].isin(val_mol_id)\n",
    "        test_mask = self.spec_df[\"mol_id\"].isin(test_mol_id)\n",
    "        prim_mask = train_mask | val_mask | test_mask\n",
    "        prim_mol_id = pd.Series(\n",
    "            list(set(train_mol_id) | set(val_mol_id) | set(test_mol_id)))\n",
    "        # note: primary can include secondary molecules in the test split\n",
    "        sec_masks = [(self.spec_df[\"dset\"] == dset) & (\n",
    "            self.spec_df[\"mol_id\"].isin(sec_mol_id)) for dset in self.secondary_dset]\n",
    "        assert (train_mask & val_mask & test_mask).sum() == 0\n",
    "        print(\"> primary\")\n",
    "        print(\"splits: train, val, test, total\")\n",
    "        print(\n",
    "            f\"spec: {train_mask.sum()}, {val_mask.sum()}, {test_mask.sum()}, {prim_mask.sum()}\")\n",
    "        print(\n",
    "            f\"mol: {len(train_mol_id)}, {len(val_mol_id)}, {len(test_mol_id)}, {len(prim_mol_id)}\")\n",
    "        if len(self.secondary_dset) > 0:\n",
    "            print(\"> secondary\")\n",
    "        for sec_idx, sec_dset in enumerate(self.secondary_dset):\n",
    "            cur_sec = self.spec_df[sec_masks[sec_idx]]\n",
    "            cur_sec_mol_id = cur_sec[\"mol_id\"]\n",
    "            cur_both_mol_mask = self.spec_df[\"mol_id\"].isin(\n",
    "                set(prim_mol_id) & set(cur_sec_mol_id))\n",
    "            cur_prim_both = self.spec_df[prim_mask & cur_both_mol_mask]\n",
    "            cur_sec_both = self.spec_df[sec_masks[sec_idx] & cur_both_mol_mask]\n",
    "            print(\n",
    "                f\"{sec_dset} spec = {cur_sec.shape[0]}, mol = {cur_sec_mol_id.nunique()}\")\n",
    "            print(\n",
    "                f\"{sec_dset} overlap: prim spec = {cur_prim_both.shape[0]}, sec spec = {cur_sec_both.shape[0]}, mol = {cur_prim_both['mol_id'].nunique()}\")\n",
    "        return train_mask, val_mask, test_mask, sec_masks\n",
    "\n",
    "    def get_spec_feats(self, spec_entry):\n",
    "\n",
    "        # convert to a dense vector\n",
    "        mol_id = th.tensor(spec_entry[\"mol_id\"]).unsqueeze(0)\n",
    "        spec_id = th.tensor(spec_entry[\"spec_id\"]).unsqueeze(0)\n",
    "        group_id = th.tensor(spec_entry[\"group_id\"]).unsqueeze(0)\n",
    "        mzs = [peak[0] for peak in spec_entry[\"peaks\"]]\n",
    "        ints = [peak[1] for peak in spec_entry[\"peaks\"]]\n",
    "        prec_mz = spec_entry[\"prec_mz\"]\n",
    "        prec_mz_bin = self.bin_func([prec_mz], None, return_index=True)[0]\n",
    "        prec_diff = max(mz - prec_mz for mz in mzs)\n",
    "        num_peaks = len(mzs)\n",
    "        bin_spec = self.transform_func(self.bin_func(mzs, ints))\n",
    "        spec = th.as_tensor(bin_spec, dtype=th.float32).unsqueeze(0)\n",
    "        col_energy = spec_entry[self.ce_key]\n",
    "        inst_type = spec_entry[\"inst_type\"]\n",
    "        prec_type = spec_entry[\"prec_type\"]\n",
    "        frag_mode = spec_entry[\"frag_mode\"]\n",
    "        charge = utils.get_charge(prec_type)\n",
    "        inst_type_idx = self.inst_type_c2i[inst_type]\n",
    "        prec_type_idx = self.prec_type_c2i[prec_type]\n",
    "        frag_mode_idx = self.frag_mode_c2i[frag_mode]\n",
    "        # same as prec_mz_bin but tensor\n",
    "        prec_mz_idx = th.tensor(\n",
    "            min(prec_mz_bin, spec.shape[1] - 1)).unsqueeze(0)\n",
    "        assert prec_mz_idx < spec.shape[1], (prec_mz_bin,\n",
    "                                             prec_mz_idx, spec.shape)\n",
    "        if self.preproc_ce == \"normalize\":\n",
    "            col_energy_meta = th.tensor(\n",
    "                [(col_energy - self.mean_ce) / (self.std_ce + utils.EPS)], dtype=th.float32)\n",
    "        elif self.preproc_ce == \"quantize\":\n",
    "            ce_bins = np.arange(0, 161, step=20)  # 8 bins\n",
    "            ce_idx = np.digitize(col_energy, bins=ce_bins, right=False)\n",
    "            col_energy_meta = th.ones([len(ce_bins) + 1], dtype=th.float32)\n",
    "            col_energy_meta[ce_idx] = 1.\n",
    "        else:\n",
    "            assert self.preproc_ce == \"none\", self.preproc_ce\n",
    "            col_energy_meta = th.tensor([col_energy], dtype=th.float32)\n",
    "        inst_type_meta = th.as_tensor(\n",
    "            utils.np_one_hot(\n",
    "                inst_type_idx,\n",
    "                num_classes=self.num_inst_type),\n",
    "            dtype=th.float32)\n",
    "        prec_type_meta = th.as_tensor(\n",
    "            utils.np_one_hot(\n",
    "                prec_type_idx,\n",
    "                num_classes=self.num_prec_type),\n",
    "            dtype=th.float32)\n",
    "        frag_mode_meta = th.as_tensor(\n",
    "            utils.np_one_hot(\n",
    "                frag_mode_idx,\n",
    "                num_classes=self.num_frag_mode),\n",
    "            dtype=th.float32)\n",
    "        spec_meta_list = [\n",
    "            col_energy_meta,\n",
    "            inst_type_meta,\n",
    "            prec_type_meta,\n",
    "            frag_mode_meta,\n",
    "            col_energy_meta]\n",
    "        spec_meta = th.cat(spec_meta_list, dim=0).unsqueeze(0)\n",
    "        spec_feats = {\n",
    "            \"spec\": spec,\n",
    "            \"prec_mz\": [prec_mz],\n",
    "            \"prec_mz_bin\": [prec_mz_bin],\n",
    "            \"prec_diff\": [prec_diff],\n",
    "            \"num_peaks\": [num_peaks],\n",
    "            \"inst_type\": [inst_type],\n",
    "            \"prec_type\": [prec_type],\n",
    "            \"frag_mode\": [frag_mode],\n",
    "            \"col_energy\": [col_energy],\n",
    "            \"charge\": [charge],\n",
    "            \"spec_meta\": spec_meta,\n",
    "            \"mol_id\": mol_id,\n",
    "            \"spec_id\": spec_id,\n",
    "            \"group_id\": group_id,\n",
    "            \"prec_mz_idx\": prec_mz_idx\n",
    "        }\n",
    "        if \"lda_topic\" in spec_entry:\n",
    "            spec_feats[\"lda_topic\"] = th.tensor(\n",
    "                spec_entry[\"lda_topic\"]).unsqueeze(0)\n",
    "        return spec_feats\n",
    "\n",
    "    def get_dataloaders(self, run_d):\n",
    "\n",
    "        val_frac = run_d[\"val_frac\"]\n",
    "        test_frac = run_d[\"test_frac\"]\n",
    "        sec_frac = run_d[\"sec_frac\"]\n",
    "        split_key = run_d[\"split_key\"]\n",
    "        split_seed = run_d[\"split_seed\"]\n",
    "        assert run_d[\"batch_size\"] % run_d[\"grad_acc_interval\"] == 0\n",
    "        batch_size = run_d[\"batch_size\"] // run_d[\"grad_acc_interval\"]\n",
    "        num_workers = run_d[\"num_workers\"]\n",
    "        pin_memory = run_d[\"pin_memory\"] if run_d[\"device\"] != \"cpu\" else False\n",
    "\n",
    "        train_mask, val_mask, test_mask, sec_masks = self.get_split_masks(\n",
    "            val_frac, test_frac, sec_frac, split_key, split_seed)\n",
    "        all_idx = np.arange(len(self))\n",
    "        # th_data.RandomSampler()\n",
    "        train_ss = TrainSubset(self, all_idx[train_mask])\n",
    "        # th_data.RandomSampler(th_data.Subset(self,all_idx[val_mask]))\n",
    "        val_ss = th_data.Subset(self, all_idx[val_mask])\n",
    "        # th_data.RandomSampler(th_data.Subset(self,all_idx[test_mask]))\n",
    "        test_ss = th_data.Subset(self, all_idx[test_mask])\n",
    "        sec_ss = [th_data.Subset(self, all_idx[sec_mask])\n",
    "                  for sec_mask in sec_masks]\n",
    "\n",
    "        collate_fn = self.get_collate_fn()\n",
    "        if len(train_ss) > 0:\n",
    "            train_dl = th_data.DataLoader(\n",
    "                train_ss,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=pin_memory,\n",
    "                shuffle=True,\n",
    "                drop_last=True  # this is to prevent single data batches that mess with batchnorm\n",
    "            )\n",
    "            train_dl_2 = th_data.DataLoader(\n",
    "                train_ss,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=pin_memory,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "        else:\n",
    "            train_dl = train_dl_2 = None\n",
    "        if len(val_ss) > 0:\n",
    "            val_dl = th_data.DataLoader(\n",
    "                val_ss,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=pin_memory,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "        else:\n",
    "            val_dl = None\n",
    "        if len(test_ss) > 0:\n",
    "            test_dl = th_data.DataLoader(\n",
    "                test_ss,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=pin_memory,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "        else:\n",
    "            test_dl = None\n",
    "        sec_dls = []\n",
    "        for ss in sec_ss:\n",
    "            dl = th_data.DataLoader(\n",
    "                ss,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=pin_memory,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            sec_dls.append(dl)\n",
    "\n",
    "        # set up dl_dict\n",
    "        dl_dict = {}\n",
    "        dl_dict[\"train\"] = train_dl\n",
    "        dl_dict[\"primary\"] = {\n",
    "            \"train\": train_dl_2,\n",
    "            \"val\": val_dl,\n",
    "            \"test\": test_dl\n",
    "        }\n",
    "        dl_dict[\"secondary\"] = {}\n",
    "        for sec_idx, sec_dset in enumerate(self.secondary_dset):\n",
    "            dl_dict[\"secondary\"][f\"{sec_dset}\"] = sec_dls[sec_idx]\n",
    "\n",
    "        # set up split_id_dict\n",
    "        split_id_dict = {}\n",
    "        split_id_dict[\"primary\"] = {}\n",
    "        split_id_dict[\"primary\"][\"train\"] = self.spec_df.iloc[all_idx[train_mask]\n",
    "                                                              ][\"spec_id\"].to_numpy()\n",
    "        split_id_dict[\"primary\"][\"val\"] = self.spec_df.iloc[all_idx[val_mask]\n",
    "                                                            ][\"spec_id\"].to_numpy()\n",
    "        split_id_dict[\"primary\"][\"test\"] = self.spec_df.iloc[all_idx[test_mask]\n",
    "                                                             ][\"spec_id\"].to_numpy()\n",
    "        split_id_dict[\"secondary\"] = {}\n",
    "        for sec_idx, sec_dset in enumerate(self.secondary_dset):\n",
    "            split_id_dict[\"secondary\"][sec_dset] = self.spec_df.iloc[all_idx[sec_masks[sec_idx]]\n",
    "                                                                     ][\"spec_id\"].to_numpy()\n",
    "\n",
    "        return dl_dict, split_id_dict\n",
    "\n",
    "    def get_track_dl(\n",
    "            self,\n",
    "            idx,\n",
    "            num_rand_idx=0,\n",
    "            topk_idx=None,\n",
    "            bottomk_idx=None,\n",
    "            other_idx=None,\n",
    "            spec_ids=None):\n",
    "\n",
    "        track_seed = 5585\n",
    "        track_dl_dict = {}\n",
    "        collate_fn = self.get_collate_fn()\n",
    "        if num_rand_idx > 0:\n",
    "            with utils.np_temp_seed(track_seed):\n",
    "                rand_idx = np.random.choice(\n",
    "                    idx, size=num_rand_idx, replace=False)\n",
    "            rand_dl = th_data.DataLoader(\n",
    "                th_data.Subset(self, rand_idx),\n",
    "                batch_size=1,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=0,\n",
    "                pin_memory=False,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            track_dl_dict[\"rand\"] = rand_dl\n",
    "        if not (topk_idx is None):\n",
    "            topk_idx = idx[topk_idx]\n",
    "            topk_dl = th_data.DataLoader(\n",
    "                th_data.Subset(self, topk_idx),\n",
    "                batch_size=1,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=0,\n",
    "                pin_memory=False,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            track_dl_dict[\"topk\"] = topk_dl\n",
    "        if not (bottomk_idx is None):\n",
    "            bottomk_idx = idx[bottomk_idx]\n",
    "            bottomk_dl = th_data.DataLoader(\n",
    "                th_data.Subset(self, bottomk_idx),\n",
    "                batch_size=1,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=0,\n",
    "                pin_memory=False,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            track_dl_dict[\"bottomk\"] = bottomk_dl\n",
    "        if not (other_idx is None):\n",
    "            other_idx = idx[other_idx]\n",
    "            other_dl = th_data.DataLoader(\n",
    "                th_data.Subset(self, other_idx),\n",
    "                batch_size=1,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=0,\n",
    "                pin_memory=False,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            track_dl_dict[\"other\"] = other_dl\n",
    "        if not (spec_ids is None):\n",
    "            # preserves order\n",
    "            spec_idx = []\n",
    "            for spec_id in spec_ids:\n",
    "                spec_idx.append(\n",
    "                    int(self.spec_df[self.spec_df[\"spec_id\"] == spec_id].index[0]))\n",
    "            spec_idx = np.array(spec_idx)\n",
    "            spec_dl = th_data.DataLoader(\n",
    "                th_data.Subset(self, spec_idx),\n",
    "                batch_size=1,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=0,\n",
    "                pin_memory=False,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            track_dl_dict[\"spec\"] = spec_dl\n",
    "        return track_dl_dict\n",
    "\n",
    " \n",
    "    def get_data_dims(self):\n",
    "\n",
    "        data = self.__getitem__(0)\n",
    "        dim_d = {}\n",
    "        if self.is_fp_dset:\n",
    "            fp_dim = data[\"fp\"].shape[1]\n",
    "        else:\n",
    "            fp_dim = -1\n",
    "        if self.is_graph_dset:\n",
    "            # node\n",
    "            if self.atom_feature_mode == \"pretrain\":\n",
    "                n_dim = -1\n",
    "            else:\n",
    "                n_dim = data[\"graph\"].ndata['h'].shape[1]\n",
    "            # edge\n",
    "            if self.bond_feature_mode == \"none\":\n",
    "                e_dim = 0\n",
    "            elif self.bond_feature_mode == \"pretrain\":\n",
    "                e_dim = -1\n",
    "            else:\n",
    "                e_dim = data[\"graph\"].edata['h'].shape[1]\n",
    "        else:\n",
    "            n_dim = e_dim = -1\n",
    "        c_dim = l_dim = -1\n",
    "        if self.spec_meta_global:\n",
    "            g_dim = data[\"spec_meta\"].shape[1]\n",
    "        else:\n",
    "            g_dim = 0  # -1\n",
    "        o_dim = data[\"spec\"].shape[1]\n",
    "        dim_d = {\n",
    "            \"fp_dim\": fp_dim,\n",
    "            \"n_dim\": n_dim,\n",
    "            \"e_dim\": e_dim,\n",
    "            \"c_dim\": c_dim,\n",
    "            \"l_dim\": l_dim,\n",
    "            \"g_dim\": g_dim,\n",
    "            \"o_dim\": o_dim\n",
    "        }\n",
    "        return dim_d\n",
    "\n",
    "    def get_collate_fn(self):\n",
    "        return MassFormerDataCollator()\n",
    "\n",
    "    def process_entry(self, spec_entry, mol):\n",
    "\n",
    "        # initialize data with shared attributes\n",
    "        spec_feats = self.get_spec_feats(spec_entry)\n",
    "        data_d = {**spec_feats}\n",
    "        data_d[\"smiles\"] = [utils.mol_to_smiles(mol)]\n",
    "        data_d[\"formula\"] = [utils.mol_to_formula(mol)]\n",
    "        graph = utils.mol2graph(mol)\n",
    "        data = utils.graph2data(graph)\n",
    "        items = preprocess_item(data)\n",
    "        data_d.update(items)\n",
    "        return data_d\n",
    "\n",
    "    def load_all(self, keys):\n",
    "\n",
    "        collate_fn = self.get_collate_fn()\n",
    "        dl = th_data.DataLoader(\n",
    "            self,\n",
    "            batch_size=100,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=min(10, len(os.sched_getaffinity(0))),\n",
    "            pin_memory=False,\n",
    "            shuffle=False,\n",
    "            drop_last=False\n",
    "        )\n",
    "        all_ds = []\n",
    "        for b_idx, b in tqdm(enumerate(dl), total=len(dl), desc=\"> load_all\"):\n",
    "            b_d = {}\n",
    "            for k in keys:\n",
    "                b_d[k] = b[k]\n",
    "            all_ds.append(b_d)\n",
    "        all_d = collate_fn(all_ds)\n",
    "        return all_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> mb_na\n",
      "> num_spec = 194, num_mol = 169, num_group = 169, num_ce_per_group = 1.1479289940828403\n"
     ]
    }
   ],
   "source": [
    "demo_dataset = BaseDataset(proc_dp=\"data/proc_demo/\", primary_dset=[\"mb_na\"], secondary_dset=[], \n",
    "                           ce_key=\"nce\", inst_type=[\"FT\"], frag_mode=[\"HCD\"], ion_mode=\"P\", process_spec_old=False,\n",
    "                           pos_prec_type=['[M+H]+', '[M+H-H2O]+', '[M+H-2H2O]+', '[M+2H]2+', '[M+H-NH3]+', \"[M+Na]+\"],\n",
    "                           preproc_ce=\"normalize\", mz_max=1000., convert_ce=False, subsample_size=0, num_entries=-1,\n",
    "                           spectrum_normalization=\"l1\", res=[1,2,3,4,5,6,7], mz_bin_res=1., ints_thresh=0., transform=\"log10over3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "{'attn_bias': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'attn_edge_type': tensor([[[[   0,    0,    0],\n",
      "          [   2,  514, 1026],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0]],\n",
      "\n",
      "         [[   2,  514, 1026],\n",
      "          [   0,    0,    0],\n",
      "          [   2,  514, 1027],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0]],\n",
      "\n",
      "         [[   0,    0,    0],\n",
      "          [   2,  514, 1027],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   5,  514, 1027]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   5,  514, 1027],\n",
      "          [   0,    0,    0]],\n",
      "\n",
      "         [[   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   5,  514, 1027],\n",
      "          [   0,    0,    0],\n",
      "          [   5,  514, 1027]],\n",
      "\n",
      "         [[   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   5,  514, 1027],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   5,  514, 1027],\n",
      "          [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "        [[[   0,    0,    0],\n",
      "          [   2,  514, 1026],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0]],\n",
      "\n",
      "         [[   2,  514, 1026],\n",
      "          [   0,    0,    0],\n",
      "          [   2,  514, 1026],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0]],\n",
      "\n",
      "         [[   0,    0,    0],\n",
      "          [   2,  514, 1026],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0]],\n",
      "\n",
      "         [[   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0]],\n",
      "\n",
      "         [[   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0]]]]), 'spatial_pos': tensor([[[1, 2, 3,  ..., 6, 5, 4],\n",
      "         [2, 1, 2,  ..., 5, 4, 3],\n",
      "         [3, 2, 1,  ..., 4, 3, 2],\n",
      "         ...,\n",
      "         [6, 5, 4,  ..., 1, 2, 3],\n",
      "         [5, 4, 3,  ..., 2, 1, 2],\n",
      "         [4, 3, 2,  ..., 3, 2, 1]],\n",
      "\n",
      "        [[1, 2, 3,  ..., 0, 0, 0],\n",
      "         [2, 1, 2,  ..., 0, 0, 0],\n",
      "         [3, 2, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]]), 'in_degree': tensor([[2, 3, 4, 3, 4, 3, 2, 4, 3, 4, 5, 2, 2, 3, 4, 4, 2, 3, 3, 4, 2, 4, 2, 3,\n",
      "         4, 3],\n",
      "        [2, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'input_nodes': tensor([[[4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4]],\n",
      "\n",
      "        [[4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]]), 'input_edges': tensor([[[[[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   3,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   3,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   3,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   3,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         [[[   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   3,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         [[[   3,  515, 1028],\n",
      "           [   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         [[[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   3,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   3,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         [[[   6,  515, 1028],\n",
      "           [   3,  515, 1028],\n",
      "           [   3,  515, 1027],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   3,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         [[[   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         [[[   3,  515, 1027],\n",
      "           [   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         [[[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         [[[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]]]]), 'out_degree': tensor([[2, 3, 4, 3, 4, 3, 2, 4, 3, 4, 5, 2, 2, 3, 4, 4, 2, 3, 3, 4, 2, 4, 2, 3,\n",
      "         4, 3],\n",
      "        [2, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'labels': tensor([-1., -1.]), 'spec': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'prec_mz': [418.0138, 164.0818], 'prec_mz_bin': [418, 164], 'prec_diff': [9.999999997489795e-05, 0.0004000000000132786], 'num_peaks': [30, 27], 'inst_type': ['FT', 'FT'], 'prec_type': ['[M+H]+', '[M+H]+'], 'frag_mode': ['HCD', 'HCD'], 'col_energy': [30.0, 60.0], 'charge': [1, 1], 'spec_meta': tensor([[-0.8063,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000, -0.8063],\n",
      "        [-0.0116,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000, -0.0116]]), 'mol_id': tensor([9861, 8990]), 'spec_id': tensor([11321, 11425]), 'group_id': tensor([0, 1]), 'prec_mz_idx': tensor([418, 164]), 'smiles': ['COc1cc(OC)n2nc(S(=O)(=O)Nc3c(Cl)ccc(C)c3Cl)nc2n1', 'COCn1nnc2ccccc21'], 'formula': ['C14H13Cl2N5O4S', 'C8H9N3O']}\n"
     ]
    }
   ],
   "source": [
    "collator = demo_dataset.get_collate_fn()\n",
    "# print(demo_dataset[5])\n",
    "# print(demo_dataset[6])\n",
    "collator_example = [demo_dataset[0], demo_dataset[1]]\n",
    "collator_result = collator(collator_example)\n",
    "print(collator_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_classes': 1, 'num_atoms': 4608, 'num_in_degree': 512, 'num_out_degree': 512, 'num_edges': 1536, 'num_spatial': 512, 'num_edge_dis': 128, 'edge_type': 'multi_hop', 'multi_hop_max_dist': 5, 'spatial_pos_max': 1024, 'max_nodes': 512, 'num_hidden_layers': 12, 'embedding_dim': 768, 'hidden_size': 768, 'ffn_embedding_dim': 768, 'num_attention_heads': 32, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'layerdrop': 0.0, 'encoder_normalize_before': False, 'pre_layernorm': False, 'apply_graphormer_init': False, 'activation_fn': 'gelu', 'embed_scale': None, 'freeze_embeddings': False, 'num_trans_layers_to_freeze': 0, 'share_input_output_embed': False, 'traceable': False, 'q_noise': 0.0, 'qn_block_size': 8, 'kdim': None, 'vdim': None, 'self_attention': True, 'bias': True, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': None, 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 1, 'pad_token_id': 0, 'eos_token_id': 2, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '', '_commit_hash': None, 'transformers_version': None}\n"
     ]
    }
   ],
   "source": [
    "from transformers import GraphormerConfig, GraphormerModel\n",
    "graphormer_config = GraphormerConfig()\n",
    "print(graphormer_config.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': True, '_parameters': OrderedDict(), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_pre_hooks': OrderedDict(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_hooks_with_kwargs': OrderedDict(), '_forward_hooks_always_called': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_forward_pre_hooks_with_kwargs': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_post_hooks': OrderedDict(), '_modules': OrderedDict([('graph_encoder', GraphormerGraphEncoder(\n",
      "  (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "  (graph_node_feature): GraphormerGraphNodeFeature(\n",
      "    (atom_encoder): Embedding(4609, 768, padding_idx=0)\n",
      "    (in_degree_encoder): Embedding(512, 768, padding_idx=0)\n",
      "    (out_degree_encoder): Embedding(512, 768, padding_idx=0)\n",
      "    (graph_token): Embedding(1, 768)\n",
      "  )\n",
      "  (graph_attn_bias): GraphormerGraphAttnBias(\n",
      "    (edge_encoder): Embedding(1537, 32, padding_idx=0)\n",
      "    (edge_dis_encoder): Embedding(131072, 1)\n",
      "    (spatial_pos_encoder): Embedding(512, 32, padding_idx=0)\n",
      "    (graph_token_virtual_distance): Embedding(1, 32)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x GraphormerGraphEncoderLayer(\n",
      "      (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "      (activation_dropout_module): Dropout(p=0.1, inplace=False)\n",
      "      (activation_fn): GELUActivation()\n",
      "      (self_attn): GraphormerMultiheadAttention(\n",
      "        (attention_dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (fc2): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")), ('lm_head_transform_weight', Linear(in_features=768, out_features=768, bias=True)), ('activation_fn', GELUActivation()), ('layer_norm', LayerNorm((768,), eps=1e-05, elementwise_affine=True))]), 'config': GraphormerConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_fn\": \"gelu\",\n",
      "  \"apply_graphormer_init\": false,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bias\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dropout\": 0.1,\n",
      "  \"edge_type\": \"multi_hop\",\n",
      "  \"embed_scale\": null,\n",
      "  \"embedding_dim\": 768,\n",
      "  \"encoder_normalize_before\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_embedding_dim\": 768,\n",
      "  \"freeze_embeddings\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"kdim\": null,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_nodes\": 512,\n",
      "  \"model_type\": \"graphormer\",\n",
      "  \"multi_hop_max_dist\": 5,\n",
      "  \"num_atoms\": 4608,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_classes\": 1,\n",
      "  \"num_edge_dis\": 128,\n",
      "  \"num_edges\": 1536,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_in_degree\": 512,\n",
      "  \"num_out_degree\": 512,\n",
      "  \"num_spatial\": 512,\n",
      "  \"num_trans_layers_to_freeze\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pre_layernorm\": false,\n",
      "  \"q_noise\": 0.0,\n",
      "  \"qn_block_size\": 8,\n",
      "  \"self_attention\": true,\n",
      "  \"share_input_output_embed\": false,\n",
      "  \"spatial_pos_max\": 1024,\n",
      "  \"traceable\": false,\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"vdim\": null\n",
      "}\n",
      ", 'name_or_path': '', 'warnings_issued': {}, 'generation_config': None, 'max_nodes': 512, 'share_input_output_embed': False, 'lm_output_learned_bias': None, 'load_softmax': True, '_is_hf_initialized': True}\n"
     ]
    }
   ],
   "source": [
    "graph_model_ex = GraphormerModel(config=graphormer_config)\n",
    "print(graph_model_ex.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_feats, out_feats, dropout=0.1):\n",
    "        super(LinearBlock, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.bn = nn.BatchNorm1d(out_feats)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(self.dropout(F.relu(self.linear(x))))\n",
    "\n",
    "\n",
    "class NeimsBlock(nn.Module):\n",
    "    \"\"\" from the NEIMS paper (uses LeakyReLU instead of ReLU) \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, dropout):\n",
    "\n",
    "        super(NeimsBlock, self).__init__()\n",
    "        bottleneck_factor = 0.5\n",
    "        bottleneck_size = int(round(bottleneck_factor * out_dim))\n",
    "        self.in_batch_norm = nn.BatchNorm1d(in_dim)\n",
    "        self.in_activation = nn.LeakyReLU()\n",
    "        self.in_linear = nn.Linear(in_dim, bottleneck_size)\n",
    "        self.out_batch_norm = nn.BatchNorm1d(bottleneck_size)\n",
    "        self.out_linear = nn.Linear(bottleneck_size, out_dim)\n",
    "        self.out_activation = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h = x\n",
    "        h = self.in_batch_norm(h)\n",
    "        h = self.in_activation(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.in_linear(h)\n",
    "        h = self.out_batch_norm(h)\n",
    "        h = self.out_activation(h)\n",
    "        h = self.out_linear(h)\n",
    "        return h\n",
    "\n",
    "def mask_prediction_by_mass(raw_prediction, prec_mass_idx, prec_mass_offset, mask_value=0.):\n",
    "    # adapted from NEIMS\n",
    "    # raw_prediction is [B,D], prec_mass_idx is [B]\n",
    "\n",
    "    max_idx = raw_prediction.shape[1]\n",
    "    assert th.all(prec_mass_idx < max_idx)\n",
    "    idx = th.arange(max_idx, device=prec_mass_idx.device)\n",
    "    mask = (\n",
    "        idx.unsqueeze(0) <= (\n",
    "            prec_mass_idx.unsqueeze(1) +\n",
    "            prec_mass_offset)).float()\n",
    "    return mask * raw_prediction + (1. - mask) * mask_value\n",
    "\n",
    "\n",
    "def reverse_prediction(raw_prediction, prec_mass_idx, prec_mass_offset):\n",
    "    # adapted from NEIMS\n",
    "    # raw_prediction is [B,D], prec_mass_idx is [B]\n",
    "\n",
    "    batch_size = raw_prediction.shape[0]\n",
    "    max_idx = raw_prediction.shape[1]\n",
    "    assert th.all(prec_mass_idx < max_idx)\n",
    "    rev_prediction = th.flip(raw_prediction, dims=(1,))\n",
    "    # convention is to shift right, so we express as negative to go left\n",
    "    offset_idx = th.minimum(\n",
    "        max_idx * th.ones_like(prec_mass_idx),\n",
    "        prec_mass_idx + prec_mass_offset + 1)\n",
    "    shifts = - (max_idx - offset_idx)\n",
    "    gather_idx = th.arange(\n",
    "        max_idx,\n",
    "        device=raw_prediction.device).unsqueeze(0).expand(\n",
    "        batch_size,\n",
    "        max_idx)\n",
    "    gather_idx = (gather_idx - shifts.unsqueeze(1)) % max_idx\n",
    "    offset_rev_prediction = th.gather(rev_prediction, 1, gather_idx)\n",
    "    # you could mask_prediction_by_mass here but it's unnecessary\n",
    "    return offset_rev_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModule(nn.Module):\n",
    "\n",
    "    # TODO: fill in the rest of the arguments\n",
    "    def __init__(self, g_dim, m_dim, o_dim, ff_h_dim, embed_linear, embed_agg, ff_layer_type, dropout, bidirectional_prediction, output_activation, output_normalization, ff_num_layers, gate_prediction, ff_skip):\n",
    "\n",
    "        super(MLPModule, self).__init__()\n",
    "\n",
    "        self.g_dim = g_dim # size of input graph embedding\n",
    "        self.m_dim = m_dim # size of input metadata\n",
    "        self.o_dim = o_dim # size of output\n",
    "        self.ff_h_dim = ff_h_dim or g_dim # size of hidden layer in feedforward network\n",
    "        self.embed_linear = embed_linear or False # whether to project inputs to ff_h_dim\n",
    "        self.embed_agg =  embed_agg or \"avg\" # aggregation method for embedding\n",
    "        self.ff_layer_type = ff_layer_type or \"neims\" # type of feedforward layer\n",
    "        self.dropout = dropout or 0.1 # dropout rate\n",
    "        self.bidirectional_prediction = bidirectional_prediction or False # whether to predict forward and reverse\n",
    "        self.output_activation = output_activation or \"relu\" # activation function for output\n",
    "        self.output_normalization = output_normalization or \"l1\" # normalization function for output\n",
    "        self.ff_num_layers = ff_num_layers or 4 # number of feedforward layers\n",
    "        self.gate_prediction = gate_prediction # whether to gate the output\n",
    "        self.ff_skip = ff_skip# whether to skip connections in feedforward layers\n",
    "        self.gt_gate_prediction = False # whether to gate the output with ground truth\n",
    "\n",
    "        if self.embed_linear:\n",
    "            # project each input to ff_h_dim\n",
    "            self.g_embed_layer = nn.Linear(self.g_dim, self.ff_h_dim)\n",
    "            self.m_embed_layer = nn.Linear(self.m_dim, self.ff_h_dim)\n",
    "            self.g_embed_dim = self.ff_h_dim\n",
    "            self.m_embed_dim = self.ff_h_dim\n",
    "        else:\n",
    "            # don't modify the inputs\n",
    "            self.g_embed_layer = nn.Identity()\n",
    "            self.m_embed_layer = nn.Identity()\n",
    "            self.g_embed_dim = self.g_dim\n",
    "            self.m_embed_dim = self.m_dim\n",
    "        if self.embed_agg == \"concat\":\n",
    "            self.embed_agg_fn = lambda x: th.cat(x, dim=1)\n",
    "            self.embed_dim = self.g_embed_dim + self.m_embed_dim\n",
    "        elif self.embed_agg == \"add\":\n",
    "            assert self.g_embed_dim == self.m_embed_dim\n",
    "            self.embed_agg_fn = lambda x: sum(x)\n",
    "            self.embed_dim = self.g_embed_dim\n",
    "        elif self.embed_agg == \"avg\":\n",
    "            print(f\"self.g_embed_dim: {self.g_embed_dim}\")\n",
    "            print(f\"self.m_embed_dim: {self.m_embed_dim}\")\n",
    "            assert self.g_embed_dim == self.m_embed_dim\n",
    "            self.embed_agg_fn = lambda x: sum(x) / len(x)\n",
    "            self.embed_dim = self.g_embed_dim\n",
    "        else:\n",
    "            raise ValueError(\"invalid agg_embed\")\n",
    "        self.ff_layers = nn.ModuleList([])\n",
    "        self.out_modules = []\n",
    "        if self.ff_layer_type == \"standard\":\n",
    "            ff_layer = LinearBlock\n",
    "        else:\n",
    "            assert self.ff_layer_type == \"neims\", self.ff_layer_type\n",
    "            ff_layer = NeimsBlock\n",
    "        self.ff_layers.append(nn.Linear(self.embed_dim, self.ff_h_dim))\n",
    "        self.out_modules.extend([\"ff_layers\"])\n",
    "        for i in range(self.ff_num_layers):\n",
    "            self.ff_layers.append(\n",
    "                ff_layer(\n",
    "                    self.ff_h_dim,\n",
    "                    self.ff_h_dim,\n",
    "                    self.dropout))\n",
    "        if self.bidirectional_prediction:\n",
    "            # assumes gating, mass masking\n",
    "            self.forw_out_layer = nn.Linear(self.ff_h_dim, self.o_dim)\n",
    "            self.rev_out_layer = nn.Linear(self.ff_h_dim, self.o_dim)\n",
    "            self.out_gate = nn.Sequential(\n",
    "                *[nn.Linear(self.ff_h_dim, self.o_dim), nn.Sigmoid()])\n",
    "        else:\n",
    "            self.out_layer = nn.Linear(self.ff_h_dim, self.o_dim)\n",
    "            if self.gate_prediction:\n",
    "                self.out_gate = nn.Sequential(\n",
    "                    *[nn.Linear(self.ff_h_dim, self.o_dim), nn.Sigmoid()])\n",
    "        # output activation\n",
    "        if self.output_activation == \"relu\":\n",
    "            self.output_activation_fn = F.relu\n",
    "        elif self.output_activation == \"sp\":\n",
    "            self.output_activation_fn = F.softplus\n",
    "        elif self.output_activation == \"sm\":\n",
    "            # you shouldn't gate with sm\n",
    "            assert not self.bidirectional_prediction\n",
    "            assert not self.gate_prediction\n",
    "            self.output_activation_fn = lambda x: F.softmax(x, dim=1)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"invalid output_activation: {self.output_activation}\")\n",
    "        # output normalization\n",
    "        if self.output_normalization == \"l1\":\n",
    "            self.output_normalization_fn = lambda x: F.normalize(x, p=1, dim=1)\n",
    "        elif self.output_normalization == \"l2\":\n",
    "            self.output_normalization_fn = lambda x: F.normalize(x, p=2, dim=1)\n",
    "        elif self.output_normalization == \"none\":\n",
    "            self.output_normalization_fn = lambda x: x\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"invalid output_normalization: {self.output_normalization}\")\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        input_embeds = []\n",
    "        # add the graph embedding\n",
    "        g_embed = self.g_embed_layer(data[\"graph_embed\"])\n",
    "        input_embeds.append(g_embed)\n",
    "        # add the metadata embedding\n",
    "        m_embed = self.m_embed_layer(data[\"spec_meta\"])\n",
    "        input_embeds.append(m_embed)\n",
    "        # aggregate\n",
    "        fh = self.embed_agg_fn(input_embeds)\n",
    "        # apply feedforward layers\n",
    "        fh = self.ff_layers[0](fh)\n",
    "        for ff_layer in self.ff_layers[1:]:\n",
    "            if self.ff_skip:\n",
    "                fh = fh + ff_layer(fh)\n",
    "            else:\n",
    "                fh = ff_layer(fh)\n",
    "        if self.bidirectional_prediction:\n",
    "            ff = self.forw_out_layer(fh)\n",
    "            fr = reverse_prediction(\n",
    "                self.rev_out_layer(fh),\n",
    "                data[\"prec_mz_idx\"],\n",
    "                self.prec_mass_offset)\n",
    "            fg = self.out_gate(fh)\n",
    "            fo = ff * fg + fr * (1. - fg)\n",
    "            fo = mask_prediction_by_mass(\n",
    "                fo, data[\"prec_mz_idx\"], self.prec_mass_offset)\n",
    "        else:\n",
    "            # apply output layer\n",
    "            fo = self.out_layer(fh)\n",
    "            # apply gating\n",
    "            if self.gate_prediction:\n",
    "                    fg = self.out_gate(fh)\n",
    "                    fo = fg * fo\n",
    "        # apply output activation\n",
    "        fo = self.output_activation_fn(fo)\n",
    "        # apply gt gating\n",
    "        if self.gt_gate_prediction:\n",
    "            # binarize gt spec\n",
    "            gt_fo = (data[\"spec\"] > 0.).float()\n",
    "            # map binary to [1-gt_gate_val,gt_gate_val]\n",
    "            assert self.gt_gate_val > 0.5\n",
    "            gt_fo = gt_fo * (2 * self.gt_gate_val - 1.) + \\\n",
    "                (1. - self.gt_gate_val)\n",
    "            # multiply\n",
    "            fo = gt_fo * fo\n",
    "        # apply normalization\n",
    "        fo = self.output_normalization_fn(fo)\n",
    "        # package\n",
    "        output_d = {\"pred\":fo}\n",
    "        return output_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MassFormer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MassFormer(nn.Module):\n",
    "\n",
    "    def __init__(self, graphormer_config):\n",
    "        super(MassFormer, self).__init__()\n",
    "        self.graphormer_module = GraphormerModel(config=graphormer_config)\n",
    "        self.mlp_module = MLPModule(\n",
    "            g_dim=768,\n",
    "            m_dim=10,  # Adjust the dimension to match tensor a\n",
    "            o_dim=1000,\n",
    "            ff_h_dim=1000,\n",
    "            embed_linear=False,\n",
    "            embed_agg=\"concat\",\n",
    "            ff_layer_type=\"neims\", \n",
    "            dropout=0.1,\n",
    "            bidirectional_prediction=False,\n",
    "            output_activation=\"relu\",\n",
    "            output_normalization=\"l1\",\n",
    "            ff_num_layers=4,\n",
    "            gate_prediction=False,\n",
    "            ff_skip=True\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_graph_data(self, graph_entry):\n",
    "        print(f\"graph_entry keys = {list(graph_entry.keys())}\")\n",
    "\n",
    "        # this list of arguments is basically what preprocess_item produces\n",
    "        gf_keys = ['input_nodes', 'attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'out_degree', 'input_edges', 'labels']\n",
    "        gf_item = {}\n",
    "        shapes = {}\n",
    "\n",
    "        for k in gf_keys:\n",
    "            # note: some keys are optional, so we need to check for them\n",
    "            if k in graph_entry:\n",
    "                gf_item[k] = graph_entry[k]\n",
    "\n",
    "                if isinstance(gf_item[k], th.Tensor):  # Check if it is a tensor\n",
    "                    shapes[k] = gf_item[k].shape # Store the shape for later\n",
    "            else:\n",
    "                # helpful debugging message\n",
    "                print(f\"Warning: {k} not found in item\")\n",
    "\n",
    "        # After the loop, print all shapes\n",
    "        print(f\"---------------- Shape of gf_items ----------------\")\n",
    "        for k, shape in shapes.items():\n",
    "            print(f\"Shape of {k}: {shape}\")    \n",
    "        return gf_item\n",
    "        \n",
    "\n",
    "    def get_spec_data(self, spec_entry):\n",
    "        print(f\"spec_entry keys = {list(spec_entry.keys())}\")\n",
    "        # remove gf-related keys\n",
    "        gf_related_keys = ['spatial_pos', 'labels', 'y', 'edge_index', 'edge_attr', \n",
    "                           'num_nodes', 'attn_bias', 'input_nodes', 'attn_edge_type', \n",
    "                           'in_degree', 'input_edges', 'out_degree', 'x']\n",
    "        for k in gf_related_keys:\n",
    "                # note: some keys are optional, so we need to check for them\n",
    "                if k in spec_entry:\n",
    "                    spec_entry.pop(k)\n",
    "                else:\n",
    "                    print(f\"Warning: {k} not found in item\")\n",
    "\n",
    "        # now, let's handle the rest of the stuff\n",
    "        spec_item = {k: [] for k in spec_entry.keys()}\n",
    "        shapes = {}  # Dictionary to store shapes\n",
    "\n",
    "        for k, v in spec_entry.items():\n",
    "                    spec_item[k] = (v)\n",
    "                    if isinstance(spec_item[k], th.Tensor):  # Check if it is a tensor\n",
    "                        shapes[k] = spec_item[k].shape  # Store the shape for later\n",
    "\n",
    "        # After the loop, print all shapes\n",
    "        print(f\"---------------- Shape of spec_item ----------------\")\n",
    "        for k, shape in shapes.items():\n",
    "            print(f\"Shape of {k}: {shape}\")    \n",
    "        return spec_item\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        graph_data = self.get_graph_data(data)\n",
    "        spec_data = self.get_spec_data(data)\n",
    "        graph_embedding = self.graphormer_module(**graph_data)\n",
    "        graph_embedding = graph_embedding.last_hidden_state[:, 0, :] # extract the embedding of the super node\n",
    "        data_dict = {\"graph_embed\": graph_embedding, **spec_data}\n",
    "        output = self.mlp_module(data_dict)\n",
    "        print(output['pred'].shape)\n",
    "        print(output[\"pred\"])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([2, 26, 1])\n",
      "Shape of attn_bias: torch.Size([2, 27, 27])\n",
      "Shape of attn_edge_type: torch.Size([2, 26, 26, 3])\n",
      "Shape of spatial_pos: torch.Size([2, 26, 26])\n",
      "Shape of in_degree: torch.Size([2, 26])\n",
      "Shape of out_degree: torch.Size([2, 26])\n",
      "Shape of input_edges: torch.Size([2, 26, 26, 12, 3])\n",
      "Shape of labels: torch.Size([2])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([2, 1000])\n",
      "Shape of spec_meta: torch.Size([2, 10])\n",
      "Shape of mol_id: torch.Size([2])\n",
      "Shape of spec_id: torch.Size([2])\n",
      "Shape of group_id: torch.Size([2])\n",
      "Shape of prec_mz_idx: torch.Size([2])\n",
      "torch.Size([2, 1000])\n",
      "tensor([[0.0000, 0.0024, 0.0015,  ..., 0.0000, 0.0000, 0.0024],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0026, 0.0028, 0.0024]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pred': tensor([[0.0000, 0.0024, 0.0015,  ..., 0.0000, 0.0000, 0.0024],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0026, 0.0028, 0.0024]],\n",
       "        grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "collated_ex = deepcopy(collator_result)\n",
    "massformer_example = MassFormer(graphormer_config)\n",
    "massformer_example.forward(collated_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> mb_na\n",
      "> num_spec = 194, num_mol = 169, num_group = 169, num_ce_per_group = 1.1479289940828403\n",
      "> primary\n",
      "splits: train, val, test, total\n",
      "spec: 111, 16, 67, 194\n",
      "mol: 100, 13, 56, 169\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 73, 1])\n",
      "Shape of attn_bias: torch.Size([32, 74, 74])\n",
      "Shape of attn_edge_type: torch.Size([32, 73, 73, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 73, 73])\n",
      "Shape of in_degree: torch.Size([32, 73])\n",
      "Shape of out_degree: torch.Size([32, 73])\n",
      "Shape of input_edges: torch.Size([32, 73, 73, 27, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000e+00, 5.3740e-03, 0.0000e+00,  ..., 7.4817e-04, 1.9430e-03,\n",
      "         2.1003e-05],\n",
      "        [0.0000e+00, 1.4787e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 3.8043e-03, 0.0000e+00,  ..., 0.0000e+00, 3.3148e-03,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 3.8406e-04,  ..., 0.0000e+00, 2.2363e-03,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.6187e-04, 0.0000e+00,  ..., 2.1692e-03, 9.2835e-04,\n",
      "         1.2972e-03],\n",
      "        [0.0000e+00, 4.1129e-04, 1.4534e-03,  ..., 1.6052e-03, 1.7219e-04,\n",
      "         1.4036e-03]], grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 57, 1])\n",
      "Shape of attn_bias: torch.Size([32, 58, 58])\n",
      "Shape of attn_edge_type: torch.Size([32, 57, 57, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 57, 57])\n",
      "Shape of in_degree: torch.Size([32, 57])\n",
      "Shape of out_degree: torch.Size([32, 57])\n",
      "Shape of input_edges: torch.Size([32, 57, 57, 16, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0001, 0.0000, 0.0000,  ..., 0.0008, 0.0002, 0.0000],\n",
      "        [0.0016, 0.0012, 0.0000,  ..., 0.0000, 0.0020, 0.0000],\n",
      "        [0.0014, 0.0042, 0.0000,  ..., 0.0010, 0.0008, 0.0012],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0028, 0.0021],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0019, 0.0000],\n",
      "        [0.0000, 0.0034, 0.0000,  ..., 0.0027, 0.0031, 0.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 40, 1])\n",
      "Shape of attn_bias: torch.Size([32, 41, 41])\n",
      "Shape of attn_edge_type: torch.Size([32, 40, 40, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 40, 40])\n",
      "Shape of in_degree: torch.Size([32, 40])\n",
      "Shape of out_degree: torch.Size([32, 40])\n",
      "Shape of input_edges: torch.Size([32, 40, 40, 15, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4077e-03, 2.4340e-03,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.3075e-03, 8.2619e-04,\n",
      "         4.6926e-04],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.6895e-03, 3.2271e-05,\n",
      "         2.0227e-03],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0426e-03, 5.3430e-04,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 6.5668e-04, 0.0000e+00,  ..., 0.0000e+00, 5.0168e-04,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 8.8519e-04,\n",
      "         1.9506e-03]], grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([16, 30, 1])\n",
      "Shape of attn_bias: torch.Size([16, 31, 31])\n",
      "Shape of attn_edge_type: torch.Size([16, 30, 30, 3])\n",
      "Shape of spatial_pos: torch.Size([16, 30, 30])\n",
      "Shape of in_degree: torch.Size([16, 30])\n",
      "Shape of out_degree: torch.Size([16, 30])\n",
      "Shape of input_edges: torch.Size([16, 30, 30, 18, 3])\n",
      "Shape of labels: torch.Size([16])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([16, 1000])\n",
      "Shape of spec_meta: torch.Size([16, 10])\n",
      "Shape of mol_id: torch.Size([16])\n",
      "Shape of spec_id: torch.Size([16])\n",
      "Shape of group_id: torch.Size([16])\n",
      "Shape of prec_mz_idx: torch.Size([16])\n",
      "torch.Size([16, 1000])\n",
      "tensor([[0.0017, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000],\n",
      "        [0.0014, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],\n",
      "        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0028, 0.0000],\n",
      "        ...,\n",
      "        [0.0017, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],\n",
      "        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],\n",
      "        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000]])\n",
      "Epoch 1/10, Val Loss: 0.8567\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 73, 1])\n",
      "Shape of attn_bias: torch.Size([32, 74, 74])\n",
      "Shape of attn_edge_type: torch.Size([32, 73, 73, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 73, 73])\n",
      "Shape of in_degree: torch.Size([32, 73])\n",
      "Shape of out_degree: torch.Size([32, 73])\n",
      "Shape of input_edges: torch.Size([32, 73, 73, 27, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000e+00, 2.3001e-03, 0.0000e+00,  ..., 0.0000e+00, 1.0941e-03,\n",
      "         6.6204e-04],\n",
      "        [0.0000e+00, 2.1353e-04, 0.0000e+00,  ..., 2.0895e-03, 2.2922e-03,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 3.1326e-03, 0.0000e+00,  ..., 2.0717e-03, 3.7021e-04,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 1.5462e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.2885e-03, 4.6856e-04,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 5.1600e-07, 0.0000e+00,  ..., 2.2310e-03, 0.0000e+00,\n",
      "         0.0000e+00]], grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 51, 1])\n",
      "Shape of attn_bias: torch.Size([32, 52, 52])\n",
      "Shape of attn_edge_type: torch.Size([32, 51, 51, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 51, 51])\n",
      "Shape of in_degree: torch.Size([32, 51])\n",
      "Shape of out_degree: torch.Size([32, 51])\n",
      "Shape of input_edges: torch.Size([32, 51, 51, 17, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1232e-03, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.9559e-04, 4.9909e-03,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.0475e-03, 0.0000e+00,  ..., 1.2470e-03, 1.6158e-03,\n",
      "         3.7038e-04],\n",
      "        ...,\n",
      "        [5.3514e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.2361e-03,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.0856e-03,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.2398e-03, 0.0000e+00,  ..., 0.0000e+00, 5.8697e-05,\n",
      "         0.0000e+00]], grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 60, 1])\n",
      "Shape of attn_bias: torch.Size([32, 61, 61])\n",
      "Shape of attn_edge_type: torch.Size([32, 60, 60, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 60, 60])\n",
      "Shape of in_degree: torch.Size([32, 60])\n",
      "Shape of out_degree: torch.Size([32, 60])\n",
      "Shape of input_edges: torch.Size([32, 60, 60, 23, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0020, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0016, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0019, 0.0001],\n",
      "        ...,\n",
      "        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0029, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([16, 30, 1])\n",
      "Shape of attn_bias: torch.Size([16, 31, 31])\n",
      "Shape of attn_edge_type: torch.Size([16, 30, 30, 3])\n",
      "Shape of spatial_pos: torch.Size([16, 30, 30])\n",
      "Shape of in_degree: torch.Size([16, 30])\n",
      "Shape of out_degree: torch.Size([16, 30])\n",
      "Shape of input_edges: torch.Size([16, 30, 30, 18, 3])\n",
      "Shape of labels: torch.Size([16])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([16, 1000])\n",
      "Shape of spec_meta: torch.Size([16, 10])\n",
      "Shape of mol_id: torch.Size([16])\n",
      "Shape of spec_id: torch.Size([16])\n",
      "Shape of group_id: torch.Size([16])\n",
      "Shape of prec_mz_idx: torch.Size([16])\n",
      "torch.Size([16, 1000])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000]])\n",
      "Epoch 2/10, Val Loss: 0.8282\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 51, 1])\n",
      "Shape of attn_bias: torch.Size([32, 52, 52])\n",
      "Shape of attn_edge_type: torch.Size([32, 51, 51, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 51, 51])\n",
      "Shape of in_degree: torch.Size([32, 51])\n",
      "Shape of out_degree: torch.Size([32, 51])\n",
      "Shape of input_edges: torch.Size([32, 51, 51, 17, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0071, 0.0000],\n",
      "        [0.0000, 0.0006, 0.0000,  ..., 0.0028, 0.0027, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0023],\n",
      "        [0.0000, 0.0003, 0.0000,  ..., 0.0007, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0025, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 57, 1])\n",
      "Shape of attn_bias: torch.Size([32, 58, 58])\n",
      "Shape of attn_edge_type: torch.Size([32, 57, 57, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 57, 57])\n",
      "Shape of in_degree: torch.Size([32, 57])\n",
      "Shape of out_degree: torch.Size([32, 57])\n",
      "Shape of input_edges: torch.Size([32, 57, 57, 16, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9855e-04, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 9.3609e-05, 0.0000e+00,  ..., 0.0000e+00, 3.0873e-03,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 2.3254e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.2208e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.2793e-04,\n",
      "         1.1834e-03]], grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 73, 1])\n",
      "Shape of attn_bias: torch.Size([32, 74, 74])\n",
      "Shape of attn_edge_type: torch.Size([32, 73, 73, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 73, 73])\n",
      "Shape of in_degree: torch.Size([32, 73])\n",
      "Shape of out_degree: torch.Size([32, 73])\n",
      "Shape of input_edges: torch.Size([32, 73, 73, 27, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4440e-04, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.8460e-05, 3.1980e-04,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.9379e-04, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.6459e-04,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]], grad_fn=<DivBackward0>)\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([16, 30, 1])\n",
      "Shape of attn_bias: torch.Size([16, 31, 31])\n",
      "Shape of attn_edge_type: torch.Size([16, 30, 30, 3])\n",
      "Shape of spatial_pos: torch.Size([16, 30, 30])\n",
      "Shape of in_degree: torch.Size([16, 30])\n",
      "Shape of out_degree: torch.Size([16, 30])\n",
      "Shape of input_edges: torch.Size([16, 30, 30, 18, 3])\n",
      "Shape of labels: torch.Size([16])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([16, 1000])\n",
      "Shape of spec_meta: torch.Size([16, 10])\n",
      "Shape of mol_id: torch.Size([16])\n",
      "Shape of spec_id: torch.Size([16])\n",
      "Shape of group_id: torch.Size([16])\n",
      "Shape of prec_mz_idx: torch.Size([16])\n",
      "torch.Size([16, 1000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 3/10, Val Loss: 0.8115\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 60, 1])\n",
      "Shape of attn_bias: torch.Size([32, 61, 61])\n",
      "Shape of attn_edge_type: torch.Size([32, 60, 60, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 60, 60])\n",
      "Shape of in_degree: torch.Size([32, 60])\n",
      "Shape of out_degree: torch.Size([32, 60])\n",
      "Shape of input_edges: torch.Size([32, 60, 60, 23, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0007, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0008, 0.0017],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 51, 1])\n",
      "Shape of attn_bias: torch.Size([32, 52, 52])\n",
      "Shape of attn_edge_type: torch.Size([32, 51, 51, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 51, 51])\n",
      "Shape of in_degree: torch.Size([32, 51])\n",
      "Shape of out_degree: torch.Size([32, 51])\n",
      "Shape of input_edges: torch.Size([32, 51, 51, 15, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 73, 1])\n",
      "Shape of attn_bias: torch.Size([32, 74, 74])\n",
      "Shape of attn_edge_type: torch.Size([32, 73, 73, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 73, 73])\n",
      "Shape of in_degree: torch.Size([32, 73])\n",
      "Shape of out_degree: torch.Size([32, 73])\n",
      "Shape of input_edges: torch.Size([32, 73, 73, 27, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([16, 30, 1])\n",
      "Shape of attn_bias: torch.Size([16, 31, 31])\n",
      "Shape of attn_edge_type: torch.Size([16, 30, 30, 3])\n",
      "Shape of spatial_pos: torch.Size([16, 30, 30])\n",
      "Shape of in_degree: torch.Size([16, 30])\n",
      "Shape of out_degree: torch.Size([16, 30])\n",
      "Shape of input_edges: torch.Size([16, 30, 30, 18, 3])\n",
      "Shape of labels: torch.Size([16])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([16, 1000])\n",
      "Shape of spec_meta: torch.Size([16, 10])\n",
      "Shape of mol_id: torch.Size([16])\n",
      "Shape of spec_id: torch.Size([16])\n",
      "Shape of group_id: torch.Size([16])\n",
      "Shape of prec_mz_idx: torch.Size([16])\n",
      "torch.Size([16, 1000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 4/10, Val Loss: 0.8048\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 37, 1])\n",
      "Shape of attn_bias: torch.Size([32, 38, 38])\n",
      "Shape of attn_edge_type: torch.Size([32, 37, 37, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 37, 37])\n",
      "Shape of in_degree: torch.Size([32, 37])\n",
      "Shape of out_degree: torch.Size([32, 37])\n",
      "Shape of input_edges: torch.Size([32, 37, 37, 15, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.3396e-04, 4.7082e-04,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 8.4613e-05,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.4617e-04,\n",
      "         0.0000e+00]], grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 73, 1])\n",
      "Shape of attn_bias: torch.Size([32, 74, 74])\n",
      "Shape of attn_edge_type: torch.Size([32, 73, 73, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 73, 73])\n",
      "Shape of in_degree: torch.Size([32, 73])\n",
      "Shape of out_degree: torch.Size([32, 73])\n",
      "Shape of input_edges: torch.Size([32, 73, 73, 27, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0009, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 60, 1])\n",
      "Shape of attn_bias: torch.Size([32, 61, 61])\n",
      "Shape of attn_edge_type: torch.Size([32, 60, 60, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 60, 60])\n",
      "Shape of in_degree: torch.Size([32, 60])\n",
      "Shape of out_degree: torch.Size([32, 60])\n",
      "Shape of input_edges: torch.Size([32, 60, 60, 23, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<DivBackward0>)\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([16, 30, 1])\n",
      "Shape of attn_bias: torch.Size([16, 31, 31])\n",
      "Shape of attn_edge_type: torch.Size([16, 30, 30, 3])\n",
      "Shape of spatial_pos: torch.Size([16, 30, 30])\n",
      "Shape of in_degree: torch.Size([16, 30])\n",
      "Shape of out_degree: torch.Size([16, 30])\n",
      "Shape of input_edges: torch.Size([16, 30, 30, 18, 3])\n",
      "Shape of labels: torch.Size([16])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([16, 1000])\n",
      "Shape of spec_meta: torch.Size([16, 10])\n",
      "Shape of mol_id: torch.Size([16])\n",
      "Shape of spec_id: torch.Size([16])\n",
      "Shape of group_id: torch.Size([16])\n",
      "Shape of prec_mz_idx: torch.Size([16])\n",
      "torch.Size([16, 1000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 5/10, Val Loss: 0.8009\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 51, 1])\n",
      "Shape of attn_bias: torch.Size([32, 52, 52])\n",
      "Shape of attn_edge_type: torch.Size([32, 51, 51, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 51, 51])\n",
      "Shape of in_degree: torch.Size([32, 51])\n",
      "Shape of out_degree: torch.Size([32, 51])\n",
      "Shape of input_edges: torch.Size([32, 51, 51, 17, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         3.9690e-05],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4908e-03, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]], grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 37, 1])\n",
      "Shape of attn_bias: torch.Size([32, 38, 38])\n",
      "Shape of attn_edge_type: torch.Size([32, 37, 37, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 37, 37])\n",
      "Shape of in_degree: torch.Size([32, 37])\n",
      "Shape of out_degree: torch.Size([32, 37])\n",
      "Shape of input_edges: torch.Size([32, 37, 37, 15, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 73, 1])\n",
      "Shape of attn_bias: torch.Size([32, 74, 74])\n",
      "Shape of attn_edge_type: torch.Size([32, 73, 73, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 73, 73])\n",
      "Shape of in_degree: torch.Size([32, 73])\n",
      "Shape of out_degree: torch.Size([32, 73])\n",
      "Shape of input_edges: torch.Size([32, 73, 73, 27, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<DivBackward0>)\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([16, 30, 1])\n",
      "Shape of attn_bias: torch.Size([16, 31, 31])\n",
      "Shape of attn_edge_type: torch.Size([16, 30, 30, 3])\n",
      "Shape of spatial_pos: torch.Size([16, 30, 30])\n",
      "Shape of in_degree: torch.Size([16, 30])\n",
      "Shape of out_degree: torch.Size([16, 30])\n",
      "Shape of input_edges: torch.Size([16, 30, 30, 18, 3])\n",
      "Shape of labels: torch.Size([16])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([16, 1000])\n",
      "Shape of spec_meta: torch.Size([16, 10])\n",
      "Shape of mol_id: torch.Size([16])\n",
      "Shape of spec_id: torch.Size([16])\n",
      "Shape of group_id: torch.Size([16])\n",
      "Shape of prec_mz_idx: torch.Size([16])\n",
      "torch.Size([16, 1000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 6/10, Val Loss: 0.7974\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 51, 1])\n",
      "Shape of attn_bias: torch.Size([32, 52, 52])\n",
      "Shape of attn_edge_type: torch.Size([32, 51, 51, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 51, 51])\n",
      "Shape of in_degree: torch.Size([32, 51])\n",
      "Shape of out_degree: torch.Size([32, 51])\n",
      "Shape of input_edges: torch.Size([32, 51, 51, 16, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0011],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 40, 1])\n",
      "Shape of attn_bias: torch.Size([32, 41, 41])\n",
      "Shape of attn_edge_type: torch.Size([32, 40, 40, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 40, 40])\n",
      "Shape of in_degree: torch.Size([32, 40])\n",
      "Shape of out_degree: torch.Size([32, 40])\n",
      "Shape of input_edges: torch.Size([32, 40, 40, 15, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 73, 1])\n",
      "Shape of attn_bias: torch.Size([32, 74, 74])\n",
      "Shape of attn_edge_type: torch.Size([32, 73, 73, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 73, 73])\n",
      "Shape of in_degree: torch.Size([32, 73])\n",
      "Shape of out_degree: torch.Size([32, 73])\n",
      "Shape of input_edges: torch.Size([32, 73, 73, 27, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<DivBackward0>)\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([16, 30, 1])\n",
      "Shape of attn_bias: torch.Size([16, 31, 31])\n",
      "Shape of attn_edge_type: torch.Size([16, 30, 30, 3])\n",
      "Shape of spatial_pos: torch.Size([16, 30, 30])\n",
      "Shape of in_degree: torch.Size([16, 30])\n",
      "Shape of out_degree: torch.Size([16, 30])\n",
      "Shape of input_edges: torch.Size([16, 30, 30, 18, 3])\n",
      "Shape of labels: torch.Size([16])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([16, 1000])\n",
      "Shape of spec_meta: torch.Size([16, 10])\n",
      "Shape of mol_id: torch.Size([16])\n",
      "Shape of spec_id: torch.Size([16])\n",
      "Shape of group_id: torch.Size([16])\n",
      "Shape of prec_mz_idx: torch.Size([16])\n",
      "torch.Size([16, 1000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 7/10, Val Loss: 0.7940\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 73, 1])\n",
      "Shape of attn_bias: torch.Size([32, 74, 74])\n",
      "Shape of attn_edge_type: torch.Size([32, 73, 73, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 73, 73])\n",
      "Shape of in_degree: torch.Size([32, 73])\n",
      "Shape of out_degree: torch.Size([32, 73])\n",
      "Shape of input_edges: torch.Size([32, 73, 73, 27, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.1996e-05, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]], grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 42, 1])\n",
      "Shape of attn_bias: torch.Size([32, 43, 43])\n",
      "Shape of attn_edge_type: torch.Size([32, 42, 42, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 42, 42])\n",
      "Shape of in_degree: torch.Size([32, 42])\n",
      "Shape of out_degree: torch.Size([32, 42])\n",
      "Shape of input_edges: torch.Size([32, 42, 42, 17, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 60, 1])\n",
      "Shape of attn_bias: torch.Size([32, 61, 61])\n",
      "Shape of attn_edge_type: torch.Size([32, 60, 60, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 60, 60])\n",
      "Shape of in_degree: torch.Size([32, 60])\n",
      "Shape of out_degree: torch.Size([32, 60])\n",
      "Shape of input_edges: torch.Size([32, 60, 60, 23, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.6647e-05, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]], grad_fn=<DivBackward0>)\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([16, 30, 1])\n",
      "Shape of attn_bias: torch.Size([16, 31, 31])\n",
      "Shape of attn_edge_type: torch.Size([16, 30, 30, 3])\n",
      "Shape of spatial_pos: torch.Size([16, 30, 30])\n",
      "Shape of in_degree: torch.Size([16, 30])\n",
      "Shape of out_degree: torch.Size([16, 30])\n",
      "Shape of input_edges: torch.Size([16, 30, 30, 18, 3])\n",
      "Shape of labels: torch.Size([16])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([16, 1000])\n",
      "Shape of spec_meta: torch.Size([16, 10])\n",
      "Shape of mol_id: torch.Size([16])\n",
      "Shape of spec_id: torch.Size([16])\n",
      "Shape of group_id: torch.Size([16])\n",
      "Shape of prec_mz_idx: torch.Size([16])\n",
      "torch.Size([16, 1000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 8/10, Val Loss: 0.7900\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 42, 1])\n",
      "Shape of attn_bias: torch.Size([32, 43, 43])\n",
      "Shape of attn_edge_type: torch.Size([32, 42, 42, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 42, 42])\n",
      "Shape of in_degree: torch.Size([32, 42])\n",
      "Shape of out_degree: torch.Size([32, 42])\n",
      "Shape of input_edges: torch.Size([32, 42, 42, 16, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0001, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 51, 1])\n",
      "Shape of attn_bias: torch.Size([32, 52, 52])\n",
      "Shape of attn_edge_type: torch.Size([32, 51, 51, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 51, 51])\n",
      "Shape of in_degree: torch.Size([32, 51])\n",
      "Shape of out_degree: torch.Size([32, 51])\n",
      "Shape of input_edges: torch.Size([32, 51, 51, 17, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 73, 1])\n",
      "Shape of attn_bias: torch.Size([32, 74, 74])\n",
      "Shape of attn_edge_type: torch.Size([32, 73, 73, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 73, 73])\n",
      "Shape of in_degree: torch.Size([32, 73])\n",
      "Shape of out_degree: torch.Size([32, 73])\n",
      "Shape of input_edges: torch.Size([32, 73, 73, 27, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<DivBackward0>)\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([16, 30, 1])\n",
      "Shape of attn_bias: torch.Size([16, 31, 31])\n",
      "Shape of attn_edge_type: torch.Size([16, 30, 30, 3])\n",
      "Shape of spatial_pos: torch.Size([16, 30, 30])\n",
      "Shape of in_degree: torch.Size([16, 30])\n",
      "Shape of out_degree: torch.Size([16, 30])\n",
      "Shape of input_edges: torch.Size([16, 30, 30, 18, 3])\n",
      "Shape of labels: torch.Size([16])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([16, 1000])\n",
      "Shape of spec_meta: torch.Size([16, 10])\n",
      "Shape of mol_id: torch.Size([16])\n",
      "Shape of spec_id: torch.Size([16])\n",
      "Shape of group_id: torch.Size([16])\n",
      "Shape of prec_mz_idx: torch.Size([16])\n",
      "torch.Size([16, 1000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 9/10, Val Loss: 0.7872\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 51, 1])\n",
      "Shape of attn_bias: torch.Size([32, 52, 52])\n",
      "Shape of attn_edge_type: torch.Size([32, 51, 51, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 51, 51])\n",
      "Shape of in_degree: torch.Size([32, 51])\n",
      "Shape of out_degree: torch.Size([32, 51])\n",
      "Shape of input_edges: torch.Size([32, 51, 51, 16, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 73, 1])\n",
      "Shape of attn_bias: torch.Size([32, 74, 74])\n",
      "Shape of attn_edge_type: torch.Size([32, 73, 73, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 73, 73])\n",
      "Shape of in_degree: torch.Size([32, 73])\n",
      "Shape of out_degree: torch.Size([32, 73])\n",
      "Shape of input_edges: torch.Size([32, 73, 73, 27, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'input_nodes', 'out_degree', 'spatial_pos', 'num_nodes', 'edge_index', 'attn_edge_type', 'y', 'labels', 'x', 'attn_bias', 'in_degree', 'input_edges']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([32, 40, 1])\n",
      "Shape of attn_bias: torch.Size([32, 41, 41])\n",
      "Shape of attn_edge_type: torch.Size([32, 40, 40, 3])\n",
      "Shape of spatial_pos: torch.Size([32, 40, 40])\n",
      "Shape of in_degree: torch.Size([32, 40])\n",
      "Shape of out_degree: torch.Size([32, 40])\n",
      "Shape of input_edges: torch.Size([32, 40, 40, 15, 3])\n",
      "Shape of labels: torch.Size([32])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([32, 1000])\n",
      "Shape of spec_meta: torch.Size([32, 10])\n",
      "Shape of mol_id: torch.Size([32])\n",
      "Shape of spec_id: torch.Size([32])\n",
      "Shape of group_id: torch.Size([32])\n",
      "Shape of prec_mz_idx: torch.Size([32])\n",
      "torch.Size([32, 1000])\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.6708e-05, 0.0000e+00,\n",
      "         0.0000e+00]], grad_fn=<DivBackward0>)\n",
      "graph_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "---------------- Shape of gf_items ----------------\n",
      "Shape of input_nodes: torch.Size([16, 30, 1])\n",
      "Shape of attn_bias: torch.Size([16, 31, 31])\n",
      "Shape of attn_edge_type: torch.Size([16, 30, 30, 3])\n",
      "Shape of spatial_pos: torch.Size([16, 30, 30])\n",
      "Shape of in_degree: torch.Size([16, 30])\n",
      "Shape of out_degree: torch.Size([16, 30])\n",
      "Shape of input_edges: torch.Size([16, 30, 30, 18, 3])\n",
      "Shape of labels: torch.Size([16])\n",
      "spec_entry keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels', 'spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "Warning: y not found in item\n",
      "Warning: edge_index not found in item\n",
      "Warning: edge_attr not found in item\n",
      "Warning: num_nodes not found in item\n",
      "Warning: x not found in item\n",
      "---------------- Shape of spec_item ----------------\n",
      "Shape of spec: torch.Size([16, 1000])\n",
      "Shape of spec_meta: torch.Size([16, 10])\n",
      "Shape of mol_id: torch.Size([16])\n",
      "Shape of spec_id: torch.Size([16])\n",
      "Shape of group_id: torch.Size([16])\n",
      "Shape of prec_mz_idx: torch.Size([16])\n",
      "torch.Size([16, 1000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 10/10, Val Loss: 0.7903\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Initialize the dataset and dataloaders\n",
    "demo_dataset = BaseDataset(proc_dp=\"data/proc_demo/\", primary_dset=[\"mb_na\"], secondary_dset=[], \n",
    "                           ce_key=\"nce\", inst_type=[\"FT\"], frag_mode=[\"HCD\"], ion_mode=\"P\", process_spec_old=False,\n",
    "                           pos_prec_type=['[M+H]+', '[M+H-H2O]+', '[M+H-2H2O]+', '[M+2H]2+', '[M+H-NH3]+', \"[M+Na]+\"],\n",
    "                           preproc_ce=\"normalize\", mz_max=1000., convert_ce=False, subsample_size=0, num_entries=-1,\n",
    "                           spectrum_normalization=\"l1\", res=[1,2,3,4,5,6,7], mz_bin_res=1., ints_thresh=0., transform=\"log10over3\")\n",
    "\n",
    "# Get dataloaders using the method provided by the BaseDataset class\n",
    "run_d = {\n",
    "    \"val_frac\": 0.1,\n",
    "    \"test_frac\": 0.2,\n",
    "    \"sec_frac\": 1.00,\n",
    "    \"split_key\": \"scaffold\",\n",
    "    \"split_seed\": 42,\n",
    "    \"batch_size\": 32,\n",
    "    \"grad_acc_interval\": 1,\n",
    "    \"num_workers\": 0,\n",
    "    \"pin_memory\": True,\n",
    "    \"device\": \"cuda\" if th.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "\n",
    "dl_dict, split_id_dict = demo_dataset.get_dataloaders(run_d)\n",
    "\n",
    "train_loader = dl_dict['train']\n",
    "val_loader = dl_dict['primary']['val']\n",
    "\n",
    "# 2. Define the model, loss function, and optimizer\n",
    "graphormer_config = GraphormerConfig()  # Assume this is set up properly\n",
    "massformer_example = MassFormer(graphormer_config)\n",
    "# Define the cosine similarity measure\n",
    "cosine_similarity = th.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "def cosine_similarity_loss(output, target):\n",
    "    # Cosine similarity is between -1 and 1. Higher is better. So we subtract from 1 to create a loss.\n",
    "    cosine_loss =  1 - th.clamp(cosine_similarity(output, target), min=0, max=1)\n",
    "    return cosine_loss.mean()\n",
    "\n",
    "optimizer = th.optim.Adam(massformer_example.parameters(), lr=1e-4)\n",
    "\n",
    "# If using a GPU\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "massformer_example.to(device)\n",
    "\n",
    "# 3. Training and validation loop\n",
    "num_epochs = 10\n",
    "# Initialize lists to keep track of losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    massformer_example.train()\n",
    "    total_train_loss = 0\n",
    "    for batch_data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Transfer data to GPU if needed\n",
    "        batch_data = {k: v.to(device) if isinstance(v, th.Tensor) else v for k, v in batch_data.items()}\n",
    "        outputs = massformer_example(batch_data)\n",
    "        # Normalize labels and predictions if they are not already\n",
    "        normalized_labels = F.normalize(batch_data['spec'].float(), p=2, dim=1)\n",
    "        normalized_preds = F.normalize(outputs['pred'], p=2, dim=1)\n",
    "        \n",
    "        loss = cosine_similarity_loss(normalized_preds, normalized_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    # Store the average training loss\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    # Convert the generator object to a list\n",
    "    val_loader = list(val_loader)\n",
    "\n",
    "    # Validation\n",
    "    massformer_example.eval()\n",
    "    total_val_loss = 0\n",
    "    with th.no_grad():\n",
    "        for batch_data in val_loader:\n",
    "            # Transfer data to GPU if needed\n",
    "            batch_data = {k: v.to(device) if isinstance(v, th.Tensor) else v for k, v in batch_data.items()}\n",
    "            outputs = massformer_example(batch_data)\n",
    "            loss = cosine_similarity_loss(outputs['pred'], batch_data['spec'].float())\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    # Store the average validation loss\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# When training is done, plot the loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Losses Over Epochs')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
