{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Demo: MassFormer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: dgl in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (1.1.2.post1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgl) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgl) (1.11.3)\n",
      "Requirement already satisfied: networkx>=2.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgl) (3.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgl) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgl) (4.66.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgl) (5.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests>=2.19.0->dgl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests>=2.19.0->dgl) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: dgllife in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (0.3.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (1.3.2)\n",
      "Requirement already satisfied: pandas in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (2.1.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (1.11.3)\n",
      "Requirement already satisfied: networkx>=2.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (3.2.1)\n",
      "Requirement already satisfied: hyperopt in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (0.2.7)\n",
      "Requirement already satisfied: joblib in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from dgllife) (1.3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests>=2.22.0->dgllife) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests>=2.22.0->dgllife) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests>=2.22.0->dgllife) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from requests>=2.22.0->dgllife) (2023.7.22)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from scikit-learn>=0.22.2->dgllife) (3.2.0)\n",
      "Requirement already satisfied: six in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from hyperopt->dgllife) (1.16.0)\n",
      "Requirement already satisfied: future in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from hyperopt->dgllife) (0.18.3)\n",
      "Requirement already satisfied: cloudpickle in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from hyperopt->dgllife) (3.0.0)\n",
      "Requirement already satisfied: py4j in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from hyperopt->dgllife) (0.10.9.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from pandas->dgllife) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from pandas->dgllife) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/yunyicheng/opt/anaconda3/envs/MF/lib/python3.11/site-packages (from pandas->dgllife) (2023.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install dgl\n",
    "%pip install dgllife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dgl\n",
    "import dgllife.utils as chemutils\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.utils.data as th_data\n",
    "import massformer.algos1\n",
    "import massformer.algos2\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data\n",
    "from transformers.models.graphormer.collating_graphormer import GraphormerDataCollator, preprocess_item\n",
    "import src.massformer.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec_df = pd.read_pickle(\"data/proc_demo/spec_df.pkl\")\n",
    "# mol_df = pd.read_pickle(\"data/proc_demo/mol_df.pkl\")\n",
    "# toy_spec_df = spec_df.sample(n=1000, replace=False, random_state=420)\n",
    "# toy_mol_df = mol_df[mol_df[\"mol_id\"].isin(toy_spec_df[\"mol_id\"])]\n",
    "# toy_spec_df.to_pickle(\"data/proc_demo/toy_spec_df.pkl\")\n",
    "# toy_mol_df.to_pickle(\"data/proc_demo/toy_mol_df.pkl\")\n",
    "# toy_mol_df = toy_mol_df.set_index(\n",
    "#             \"mol_id\", drop=False).sort_index().rename_axis(None)\n",
    "# print(toy_spec_df)\n",
    "# print(toy_mol_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(toy_spec_df)\n",
    "# print(toy_mol_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "class MassFormerDataCollator(GraphormerDataCollator):\n",
    "\n",
    "    def __init__(self, spatial_pos_max=1024):\n",
    "\n",
    "        super().__init__(spatial_pos_max=spatial_pos_max, on_the_fly_processing=False)\n",
    "        # custom init stuff for MassFormer\n",
    "\n",
    "    def __call__(self, items: List[dict]):\n",
    "\n",
    "        print(f\"all keys = {list(items[0].keys())}\")\n",
    "\n",
    "        # this list of arguments is basically what preprocess_item produces\n",
    "        gf_keys = ['input_nodes', 'attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'out_degree', 'input_edges', 'labels']\n",
    "        gf_related_keys = ['spatial_pos', 'labels', 'y', 'edge_index', 'edge_attr', 'num_nodes', 'attn_bias', 'input_nodes', 'attn_edge_type', 'in_degree', 'input_edges', 'out_degree', 'x']\n",
    "        gf_items = []\n",
    "        for item in items:\n",
    "            gf_item = {}\n",
    "            for k in gf_keys:\n",
    "                # note: some keys are optional, so we need to check for them\n",
    "                if k in item:\n",
    "                    gf_item[k] = item.pop(k)\n",
    "                else:\n",
    "                    # helpful debugging message\n",
    "                    print(f\"Warning: {k} not found in item\")\n",
    "            gf_items.append(gf_item)\n",
    "\n",
    "\n",
    "        # custom call stuff for MassFormer\n",
    "        gf_collated = super().__call__(gf_items)\n",
    "        print(f\"gf_collated_keys = {list(gf_collated.keys())}\")\n",
    "\n",
    "        # After collating gf aspects, remove gf-related keys for mass spec collation\n",
    "        for item in items:\n",
    "            for k in gf_related_keys:\n",
    "                # note: some keys are optional, so we need to check for them\n",
    "                if k in item:\n",
    "                    item.pop(k)\n",
    "                else:\n",
    "                    print(f\"Warning: {k} not found in item\")\n",
    "\n",
    "        # now, let's handle the rest of the stuff\n",
    "        other_collated = {k: [] for k in items[0].keys()}\n",
    "        for data_d in items:\n",
    "            for k, v in data_d.items():\n",
    "                    other_collated[k].append(v)\n",
    "        for k, v in other_collated.items():\n",
    "            if isinstance(items[0][k], th.Tensor):\n",
    "                other_collated[k] = th.cat(v, dim=0)\n",
    "            elif isinstance(items[0][k], list):\n",
    "                other_collated[k] = utils.flatten_lol(v)\n",
    "            else:\n",
    "                raise ValueError(f\"{type(items[0][k])} is not supported\")\n",
    "\n",
    "        print(f\"other_collated_keys = {list(other_collated.keys())}\")\n",
    "\n",
    "        # now, let's combine the two collated dicts\n",
    "        both_collated = {**gf_collated, **other_collated}\n",
    "\n",
    "        return both_collated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainSubset(th_data.Subset):\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset.__getitem__(self.indices[idx])\n",
    "    \n",
    "class BaseDataset(th_data.Dataset):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "        assert os.path.isdir(self.proc_dp), self.proc_dp\n",
    "        self.spec_df = pd.read_pickle(\n",
    "            os.path.join(self.proc_dp, \"spec_df.pkl\"))\n",
    "        self.mol_df = pd.read_pickle(\n",
    "            os.path.join(self.proc_dp, \"mol_df.pkl\"))\n",
    "        self.mol_df = self.mol_df[self.mol_df[\"mol_id\"].isin(self.spec_df[\"mol_id\"])]\n",
    "\n",
    "        # generate toy spec and mol df for faster processing\n",
    "        self.toy_spec_df = self.spec_df.sample(n=1000, replace=False, random_state=420)\n",
    "        self.toy_mol_df = self.mol_df[self.mol_df[\"mol_id\"].isin(self.toy_spec_df[\"mol_id\"])]\n",
    "\n",
    "        # save toy spec and mol df as pickle\n",
    "        self.toy_spec_df.to_pickle(\"data/proc_demo/toy_spec_df.pkl\")\n",
    "        self.toy_mol_df.to_pickle(\"data/proc_demo/toy_mol_df.pkl\")\n",
    "\n",
    "        # use toy df\n",
    "        self.spec_df = self.toy_spec_df\n",
    "        self.mol_df = self.toy_mol_df\n",
    "\n",
    "\n",
    "        self._select_spec()\n",
    "        self._setup_spec_metadata_dicts()\n",
    "        # use mol_id as index for speedy access\n",
    "        self.mol_df = self.mol_df.set_index(\n",
    "            \"mol_id\", drop=False).sort_index().rename_axis(None)\n",
    "\n",
    "        # spec_df = pd.read_pickle(\"data/proc_demo/spec_df.pkl\")\n",
    "        # mol_df = pd.read_pickle(\"data/proc_demo/mol_df.pkl\")\n",
    "        # toy_spec_df = spec_df.sample(n=1000, replace=False, random_state=420)\n",
    "        # toy_mol_df = mol_df[mol_df[\"mol_id\"].isin(toy_spec_df[\"mol_id\"])]\n",
    "        # toy_spec_df.to_pickle(\"data/proc_demo/toy_spec_df.pkl\")\n",
    "        # toy_mol_df.to_pickle(\"data/proc_demo/toy_mol_df.pkl\")\n",
    "        # toy_mol_df = toy_mol_df.set_index(\n",
    "        #             \"mol_id\", drop=False).sort_index().rename_axis(None)\n",
    "\n",
    "    def _select_spec(self):\n",
    "\n",
    "        masks = []\n",
    "        # dataset mask\n",
    "        dset_mask = self.spec_df[\"dset\"].isin(\n",
    "            self.primary_dset + self.secondary_dset)\n",
    "        masks.append(dset_mask)\n",
    "        # instrument type\n",
    "        inst_type_mask = self.spec_df[\"inst_type\"].isin(self.inst_type)\n",
    "        masks.append(inst_type_mask)\n",
    "        # frag mode\n",
    "        frag_mode_mask = self.spec_df[\"frag_mode\"].isin(self.frag_mode)\n",
    "        masks.append(frag_mode_mask)\n",
    "        # ion mode\n",
    "        ion_mode_mask = self.spec_df[\"ion_mode\"] == self.ion_mode\n",
    "        masks.append(ion_mode_mask)\n",
    "        # precursor type\n",
    "        if self.ion_mode == \"P\":\n",
    "            prec_type_mask = self.spec_df[\"prec_type\"].isin(self.pos_prec_type)\n",
    "        elif self.ion_mode == \"N\":\n",
    "            prec_type_mask = self.spec_df[\"prec_type\"].isin(self.neg_prec_type)\n",
    "        else:\n",
    "            assert self.ion_mode == \"EI\"\n",
    "            prec_type_mask = self.spec_df[\"prec_type\"] == \"EI\"\n",
    "        masks.append(prec_type_mask)\n",
    "        # resolution\n",
    "        if self.res != []:\n",
    "            res_mask = self.spec_df[\"res\"].isin(self.res)\n",
    "            masks.append(res_mask)\n",
    "        # collision energy\n",
    "        ce_mask = ~(self.spec_df[\"ace\"].isna() & self.spec_df[\"nce\"].isna())\n",
    "        masks.append(ce_mask)\n",
    "        # spectrum type\n",
    "        if self.ion_mode == \"EI\":\n",
    "            spec_type_mask = self.spec_df[\"spec_type\"] == \"EI\"\n",
    "        else:\n",
    "            spec_type_mask = self.spec_df[\"spec_type\"] == \"MS2\"\n",
    "        masks.append(spec_type_mask)\n",
    "        # maximum mz allowed\n",
    "        mz_mask = self.spec_df[\"peaks\"].apply(\n",
    "            lambda peaks: max(peak[0] for peak in peaks) < self.mz_max)\n",
    "        masks.append(mz_mask)\n",
    "        # precursor mz\n",
    "        prec_mz_mask = ~self.spec_df[\"prec_mz\"].isna()\n",
    "        masks.append(prec_mz_mask)\n",
    "        # single molecule\n",
    "        multi_mol_ids = self.mol_df[self.mol_df[\"smiles\"].str.contains(\n",
    "            \"\\\\.\")][\"mol_id\"]\n",
    "        single_mol_mask = ~self.spec_df[\"mol_id\"].isin(multi_mol_ids)\n",
    "        masks.append(single_mol_mask)\n",
    "        # neutral molecule\n",
    "        charges = self.mol_df[\"mol\"].apply(utils.mol_to_charge)\n",
    "        charged_ids = self.mol_df[charges != 0][\"mol_id\"]\n",
    "        neutral_mask = ~self.spec_df[\"mol_id\"].isin(charged_ids)\n",
    "        # print(neutral_mask.sum())\n",
    "        masks.append(neutral_mask)\n",
    "        # put them together\n",
    "        all_mask = masks[0]\n",
    "        for mask in masks:\n",
    "            all_mask = all_mask & mask\n",
    "        if np.sum(all_mask) == 0:\n",
    "            raise ValueError(\"select removed all items\")\n",
    "        self.spec_df = self.spec_df[all_mask].reset_index(drop=True)\n",
    "        self._setup_ce()\n",
    "        # get group_id\n",
    "        n_before_group = self.spec_df.shape[0]\n",
    "        group_df = self.spec_df.drop(\n",
    "            columns=[\n",
    "                \"spec_id\",\n",
    "                \"peaks\",\n",
    "                \"nce\",\n",
    "                \"ace\",\n",
    "                \"res\",\n",
    "                \"prec_mz\",\n",
    "                \"ri\",\n",
    "                \"col_gas\"])\n",
    "        assert not group_df.isna().any().any()\n",
    "        group_df = group_df.drop_duplicates()\n",
    "        group_df.loc[:, \"group_id\"] = np.arange(group_df.shape[0])\n",
    "        self.spec_df = self.spec_df.merge(group_df, how=\"inner\")\n",
    "        del group_df\n",
    "        n_after_group = self.spec_df.shape[0]\n",
    "        assert n_before_group == n_after_group\n",
    "        # compute stats\n",
    "        for dset in self.primary_dset + self.secondary_dset:\n",
    "            dset_spec_df = self.spec_df[self.spec_df[\"dset\"] == dset]\n",
    "            num_specs = dset_spec_df[\"spec_id\"].nunique()\n",
    "            num_mols = dset_spec_df[\"mol_id\"].nunique()\n",
    "            num_groups = dset_spec_df[\"group_id\"].nunique()\n",
    "            num_ces_per_group = dset_spec_df[[\"group_id\", self.ce_key]].groupby(\n",
    "                by=\"group_id\").count()[self.ce_key].mean()\n",
    "            print(f\">>> {dset}\")\n",
    "            print(\n",
    "                f\"> num_spec = {num_specs}, num_mol = {num_mols}, num_group = {num_groups}, num_ce_per_group = {num_ces_per_group}\")\n",
    "        # subsample\n",
    "        if self.subsample_size > 0:\n",
    "            self.spec_df = self.spec_df.groupby(\"mol_id\").sample(\n",
    "                n=self.subsample_size, random_state=self.subsample_seed, replace=True)\n",
    "            self.spec_df = self.spec_df.reset_index(drop=True)\n",
    "        else:\n",
    "            self.spec_df = self.spec_df\n",
    "        # num_entries\n",
    "        if self.num_entries > 0:\n",
    "            self.spec_df = self.spec_df.sample(\n",
    "                n=self.num_entries,\n",
    "                random_state=self.subsample_seed,\n",
    "                replace=False)\n",
    "            self.spec_df = self.spec_df.reset_index(drop=True)\n",
    "        # only keep mols with spectra\n",
    "        self.mol_df = self.mol_df[self.mol_df[\"mol_id\"].isin(\n",
    "            self.spec_df[\"mol_id\"])]\n",
    "        self.mol_df = self.mol_df.reset_index(drop=True)\n",
    "\n",
    "    def _setup_ce(self):\n",
    "\n",
    "        if self.convert_ce:\n",
    "            if self.ce_key == \"ace\":\n",
    "                other_ce_key = \"nce\"\n",
    "                ce_conversion_fn = utils.nce_to_ace\n",
    "            else:\n",
    "                other_ce_key = \"ace\"\n",
    "                ce_conversion_fn = utils.ace_to_nce\n",
    "            convert_mask = self.spec_df[self.ce_key].isna()\n",
    "            assert not self.spec_df.loc[convert_mask,\n",
    "                                        other_ce_key].isna().any()\n",
    "            self.spec_df.loc[convert_mask, self.ce_key] = self.spec_df[convert_mask].apply(\n",
    "                ce_conversion_fn, axis=1)\n",
    "            assert not self.spec_df[self.ce_key].isna().any()\n",
    "        else:\n",
    "            self.spec_df = self.spec_df.dropna(\n",
    "                axis=0, subset=[\n",
    "                    self.ce_key]).reset_index(\n",
    "                drop=True)\n",
    "\n",
    "    def _setup_spec_metadata_dicts(self):\n",
    "\n",
    "        # featurize spectral metadata\n",
    "        # we can assume that the dataset is filtered (using the method above)\n",
    "        # to only include these values\n",
    "        inst_type_list = self.inst_type\n",
    "        if self.ion_mode == \"P\":\n",
    "            prec_type_list = self.pos_prec_type\n",
    "        elif self.ion_mode == \"N\":\n",
    "            prec_type_list = self.neg_prec_type\n",
    "        else:\n",
    "            assert self.ion_mode == \"EI\"\n",
    "            prec_type_list = [\"EI\"]\n",
    "        frag_mode_list = self.frag_mode\n",
    "        self.inst_type_c2i = {\n",
    "            string: i for i,\n",
    "            string in enumerate(inst_type_list)}\n",
    "        self.inst_type_i2c = {\n",
    "            i: string for i,\n",
    "            string in enumerate(inst_type_list)}\n",
    "        self.prec_type_c2i = {\n",
    "            string: i for i,\n",
    "            string in enumerate(prec_type_list)}\n",
    "        self.prec_type_i2c = {\n",
    "            i: string for i,\n",
    "            string in enumerate(prec_type_list)}\n",
    "        self.frag_mode_c2i = {\n",
    "            string: i for i,\n",
    "            string in enumerate(frag_mode_list)}\n",
    "        self.frag_mode_i2c = {\n",
    "            i: string for i,\n",
    "            string in enumerate(frag_mode_list)}\n",
    "        self.num_inst_type = len(inst_type_list)\n",
    "        self.num_prec_type = len(prec_type_list)\n",
    "        self.num_frag_mode = len(frag_mode_list)\n",
    "        self.max_ce = self.spec_df[self.ce_key].max()\n",
    "        self.mean_ce = self.spec_df[self.ce_key].mean()\n",
    "        self.std_ce = self.spec_df[self.ce_key].std()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        spec_entry = self.spec_df.iloc[idx]\n",
    "        mol_id = spec_entry[\"mol_id\"]\n",
    "        # mol_entry = self.mol_df[self.mol_df[\"mol_id\"] == mol_id].iloc[0]\n",
    "        mol_entry = self.mol_df.loc[mol_id]\n",
    "        data = self.process_entry(spec_entry, mol_entry[\"mol\"])\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.spec_df.shape[0]\n",
    "\n",
    "    def bin_func(self, mzs, ints, return_index=False):\n",
    "\n",
    "        assert self.ints_thresh == 0., self.ints_thresh\n",
    "        return utils.bin_func(\n",
    "            mzs,\n",
    "            ints,\n",
    "            self.mz_max,\n",
    "            self.mz_bin_res,\n",
    "            self.ints_thresh,\n",
    "            return_index)\n",
    "\n",
    "    def transform_func(self, spec):\n",
    "\n",
    "        if self.process_spec_old:\n",
    "            spec = utils.process_spec_old(\n",
    "                spec,\n",
    "                self.transform,\n",
    "                self.spectrum_normalization,\n",
    "                self.ints_thresh)\n",
    "        else:\n",
    "            spec = utils.process_spec(\n",
    "                th.as_tensor(spec),\n",
    "                self.transform,\n",
    "                self.spectrum_normalization)\n",
    "            spec = spec.numpy()\n",
    "        return spec\n",
    "\n",
    "    def get_split_masks(\n",
    "            self,\n",
    "            val_frac,\n",
    "            test_frac,\n",
    "            sec_frac,\n",
    "            split_key,\n",
    "            split_seed,\n",
    "            ignore_casmi):\n",
    "\n",
    "        assert split_key in [\"inchikey_s\", \"scaffold\"], split_key\n",
    "        assert len(self.secondary_dset) <= 1, self.secondary_dset\n",
    "        # primary\n",
    "        prim_mask = self.spec_df[\"dset\"].isin(self.primary_dset)\n",
    "        prim_mol_id = self.spec_df[prim_mask][\"mol_id\"].unique()\n",
    "        prim_key = set(\n",
    "            self.mol_df[self.mol_df[\"mol_id\"].isin(prim_mol_id)][split_key])\n",
    "        # secondary\n",
    "        sec_mask = self.spec_df[\"dset\"].isin(self.secondary_dset)\n",
    "        sec_mol_id = self.spec_df[sec_mask][\"mol_id\"].unique()\n",
    "        sec_key = set(\n",
    "            self.mol_df[self.mol_df[\"mol_id\"].isin(sec_mol_id)][split_key])\n",
    "        sec_key_list = sorted(list(sec_key))\n",
    "        # print(sec_key_list[:5])\n",
    "        # sample secondary keys\n",
    "        with utils.np_temp_seed(split_seed):\n",
    "            sec_num = round(len(sec_key_list) * sec_frac)\n",
    "            sec_key_list = np.random.choice(\n",
    "                sec_key_list, size=sec_num, replace=False).tolist()\n",
    "            sec_key = set(sec_key_list)\n",
    "            # print(sec_key_list[:5])\n",
    "            sec_mol_id = self.mol_df[self.mol_df[split_key].isin(\n",
    "                sec_key_list) & self.mol_df[\"mol_id\"].isin(sec_mol_id)][\"mol_id\"].unique()\n",
    "            sec_mask = self.spec_df[\"mol_id\"].isin(sec_mol_id) & sec_mask\n",
    "            # print(split_seed,sec_num,sec_mask.sum())\n",
    "        # get keys (secondary might same compounds as primary does!)\n",
    "        prim_only_key = prim_key - sec_key\n",
    "        sec_only_key = sec_key\n",
    "        prim_only_key_list = sorted(list(prim_only_key))\n",
    "        both_key = prim_key & sec_key\n",
    "        # compute number for each split\n",
    "        test_num = round(len(prim_only_key_list) * test_frac)\n",
    "        val_num = round(len(prim_only_key_list) * val_frac)\n",
    "        # make sure that test set gets all of the casmi keys!\n",
    "        if not ignore_casmi:\n",
    "            prim_only_and_casmi = prim_only_key & self.casmi_info[split_key]\n",
    "        else:\n",
    "            prim_only_and_casmi = set()\n",
    "        if test_num > 0:\n",
    "            assert len(prim_only_and_casmi) <= test_num\n",
    "            test_num -= len(prim_only_and_casmi)\n",
    "        prim_only_no_casmi_key_list = [\n",
    "            k for k in prim_only_key_list if not (\n",
    "                k in prim_only_and_casmi)]\n",
    "        assert len(set(prim_only_no_casmi_key_list) & prim_only_and_casmi) == 0\n",
    "        # do the split\n",
    "        with utils.np_temp_seed(split_seed):\n",
    "            prim_only_test_num = max(test_num - len(both_key), 0)\n",
    "            test_key = set(\n",
    "                np.random.choice(\n",
    "                    prim_only_no_casmi_key_list,\n",
    "                    size=prim_only_test_num,\n",
    "                    replace=False))\n",
    "            test_key = test_key.union(prim_only_and_casmi).union(both_key)\n",
    "            train_val_key = prim_only_key - test_key\n",
    "            val_key = set(\n",
    "                np.random.choice(\n",
    "                    sorted(\n",
    "                        list(train_val_key)),\n",
    "                    size=val_num,\n",
    "                    replace=False))\n",
    "            train_key = train_val_key - val_key\n",
    "            assert len(train_key & sec_only_key) == 0\n",
    "            assert len(val_key & sec_only_key) == 0\n",
    "            # assert len(test_key & sec_only_key) == 0\n",
    "            assert len(train_key & prim_only_and_casmi) == 0\n",
    "            assert len(val_key & prim_only_and_casmi) == 0\n",
    "        # get ids and create masks\n",
    "        train_mol_id = self.mol_df[\"mol_id\"][self.mol_df[split_key].isin(\n",
    "            list(train_key))].unique()\n",
    "        val_mol_id = self.mol_df[\"mol_id\"][self.mol_df[split_key].isin(\n",
    "            list(val_key))].unique()\n",
    "        test_mol_id = self.mol_df[\"mol_id\"][self.mol_df[split_key].isin(\n",
    "            list(test_key))].unique()\n",
    "        train_mask = self.spec_df[\"mol_id\"].isin(train_mol_id)\n",
    "        val_mask = self.spec_df[\"mol_id\"].isin(val_mol_id)\n",
    "        test_mask = self.spec_df[\"mol_id\"].isin(test_mol_id)\n",
    "        prim_mask = train_mask | val_mask | test_mask\n",
    "        prim_mol_id = pd.Series(\n",
    "            list(set(train_mol_id) | set(val_mol_id) | set(test_mol_id)))\n",
    "        # note: primary can include secondary molecules in the test split\n",
    "        sec_masks = [(self.spec_df[\"dset\"] == dset) & (\n",
    "            self.spec_df[\"mol_id\"].isin(sec_mol_id)) for dset in self.secondary_dset]\n",
    "        assert (train_mask & val_mask & test_mask).sum() == 0\n",
    "        print(\"> primary\")\n",
    "        print(\"splits: train, val, test, total\")\n",
    "        print(\n",
    "            f\"spec: {train_mask.sum()}, {val_mask.sum()}, {test_mask.sum()}, {prim_mask.sum()}\")\n",
    "        print(\n",
    "            f\"mol: {len(train_mol_id)}, {len(val_mol_id)}, {len(test_mol_id)}, {len(prim_mol_id)}\")\n",
    "        if len(self.secondary_dset) > 0:\n",
    "            print(\"> secondary\")\n",
    "        for sec_idx, sec_dset in enumerate(self.secondary_dset):\n",
    "            cur_sec = self.spec_df[sec_masks[sec_idx]]\n",
    "            cur_sec_mol_id = cur_sec[\"mol_id\"]\n",
    "            cur_both_mol_mask = self.spec_df[\"mol_id\"].isin(\n",
    "                set(prim_mol_id) & set(cur_sec_mol_id))\n",
    "            cur_prim_both = self.spec_df[prim_mask & cur_both_mol_mask]\n",
    "            cur_sec_both = self.spec_df[sec_masks[sec_idx] & cur_both_mol_mask]\n",
    "            print(\n",
    "                f\"{sec_dset} spec = {cur_sec.shape[0]}, mol = {cur_sec_mol_id.nunique()}\")\n",
    "            print(\n",
    "                f\"{sec_dset} overlap: prim spec = {cur_prim_both.shape[0]}, sec spec = {cur_sec_both.shape[0]}, mol = {cur_prim_both['mol_id'].nunique()}\")\n",
    "        return train_mask, val_mask, test_mask, sec_masks\n",
    "\n",
    "    def get_spec_feats(self, spec_entry):\n",
    "\n",
    "        # convert to a dense vector\n",
    "        mol_id = th.tensor(spec_entry[\"mol_id\"]).unsqueeze(0)\n",
    "        spec_id = th.tensor(spec_entry[\"spec_id\"]).unsqueeze(0)\n",
    "        group_id = th.tensor(spec_entry[\"group_id\"]).unsqueeze(0)\n",
    "        mzs = [peak[0] for peak in spec_entry[\"peaks\"]]\n",
    "        ints = [peak[1] for peak in spec_entry[\"peaks\"]]\n",
    "        prec_mz = spec_entry[\"prec_mz\"]\n",
    "        prec_mz_bin = self.bin_func([prec_mz], None, return_index=True)[0]\n",
    "        prec_diff = max(mz - prec_mz for mz in mzs)\n",
    "        num_peaks = len(mzs)\n",
    "        bin_spec = self.transform_func(self.bin_func(mzs, ints))\n",
    "        spec = th.as_tensor(bin_spec, dtype=th.float32).unsqueeze(0)\n",
    "        col_energy = spec_entry[self.ce_key]\n",
    "        inst_type = spec_entry[\"inst_type\"]\n",
    "        prec_type = spec_entry[\"prec_type\"]\n",
    "        frag_mode = spec_entry[\"frag_mode\"]\n",
    "        charge = utils.get_charge(prec_type)\n",
    "        inst_type_idx = self.inst_type_c2i[inst_type]\n",
    "        prec_type_idx = self.prec_type_c2i[prec_type]\n",
    "        frag_mode_idx = self.frag_mode_c2i[frag_mode]\n",
    "        # same as prec_mz_bin but tensor\n",
    "        prec_mz_idx = th.tensor(\n",
    "            min(prec_mz_bin, spec.shape[1] - 1)).unsqueeze(0)\n",
    "        assert prec_mz_idx < spec.shape[1], (prec_mz_bin,\n",
    "                                             prec_mz_idx, spec.shape)\n",
    "        if self.preproc_ce == \"normalize\":\n",
    "            col_energy_meta = th.tensor(\n",
    "                [(col_energy - self.mean_ce) / (self.std_ce + utils.EPS)], dtype=th.float32)\n",
    "        elif self.preproc_ce == \"quantize\":\n",
    "            ce_bins = np.arange(0, 161, step=20)  # 8 bins\n",
    "            ce_idx = np.digitize(col_energy, bins=ce_bins, right=False)\n",
    "            col_energy_meta = th.ones([len(ce_bins) + 1], dtype=th.float32)\n",
    "            col_energy_meta[ce_idx] = 1.\n",
    "        else:\n",
    "            assert self.preproc_ce == \"none\", self.preproc_ce\n",
    "            col_energy_meta = th.tensor([col_energy], dtype=th.float32)\n",
    "        inst_type_meta = th.as_tensor(\n",
    "            utils.np_one_hot(\n",
    "                inst_type_idx,\n",
    "                num_classes=self.num_inst_type),\n",
    "            dtype=th.float32)\n",
    "        prec_type_meta = th.as_tensor(\n",
    "            utils.np_one_hot(\n",
    "                prec_type_idx,\n",
    "                num_classes=self.num_prec_type),\n",
    "            dtype=th.float32)\n",
    "        frag_mode_meta = th.as_tensor(\n",
    "            utils.np_one_hot(\n",
    "                frag_mode_idx,\n",
    "                num_classes=self.num_frag_mode),\n",
    "            dtype=th.float32)\n",
    "        spec_meta_list = [\n",
    "            col_energy_meta,\n",
    "            inst_type_meta,\n",
    "            prec_type_meta,\n",
    "            frag_mode_meta,\n",
    "            col_energy_meta]\n",
    "        spec_meta = th.cat(spec_meta_list, dim=0).unsqueeze(0)\n",
    "        spec_feats = {\n",
    "            \"spec\": spec,\n",
    "            \"prec_mz\": [prec_mz],\n",
    "            \"prec_mz_bin\": [prec_mz_bin],\n",
    "            \"prec_diff\": [prec_diff],\n",
    "            \"num_peaks\": [num_peaks],\n",
    "            \"inst_type\": [inst_type],\n",
    "            \"prec_type\": [prec_type],\n",
    "            \"frag_mode\": [frag_mode],\n",
    "            \"col_energy\": [col_energy],\n",
    "            \"charge\": [charge],\n",
    "            \"spec_meta\": spec_meta,\n",
    "            \"mol_id\": mol_id,\n",
    "            \"spec_id\": spec_id,\n",
    "            \"group_id\": group_id,\n",
    "            \"prec_mz_idx\": prec_mz_idx\n",
    "        }\n",
    "        if \"casmi_id\" in spec_entry:\n",
    "            spec_feats[\"casmi_id\"] = th.tensor(\n",
    "                spec_entry[\"casmi_id\"]).unsqueeze(0)\n",
    "        if \"lda_topic\" in spec_entry:\n",
    "            spec_feats[\"lda_topic\"] = th.tensor(\n",
    "                spec_entry[\"lda_topic\"]).unsqueeze(0)\n",
    "        return spec_feats\n",
    "\n",
    "    def get_dataloaders(self, run_d):\n",
    "\n",
    "        val_frac = run_d[\"val_frac\"]\n",
    "        test_frac = run_d[\"test_frac\"]\n",
    "        sec_frac = run_d[\"sec_frac\"]\n",
    "        split_key = run_d[\"split_key\"]\n",
    "        split_seed = run_d[\"split_seed\"]\n",
    "        ignore_casmi = run_d[\"ignore_casmi_in_split\"]\n",
    "        assert run_d[\"batch_size\"] % run_d[\"grad_acc_interval\"] == 0\n",
    "        batch_size = run_d[\"batch_size\"] // run_d[\"grad_acc_interval\"]\n",
    "        num_workers = run_d[\"num_workers\"]\n",
    "        pin_memory = run_d[\"pin_memory\"] if run_d[\"device\"] != \"cpu\" else False\n",
    "\n",
    "        train_mask, val_mask, test_mask, sec_masks = self.get_split_masks(\n",
    "            val_frac, test_frac, sec_frac, split_key, split_seed, ignore_casmi)\n",
    "        all_idx = np.arange(len(self))\n",
    "        # th_data.RandomSampler()\n",
    "        train_ss = TrainSubset(self, all_idx[train_mask])\n",
    "        # th_data.RandomSampler(th_data.Subset(self,all_idx[val_mask]))\n",
    "        val_ss = th_data.Subset(self, all_idx[val_mask])\n",
    "        # th_data.RandomSampler(th_data.Subset(self,all_idx[test_mask]))\n",
    "        test_ss = th_data.Subset(self, all_idx[test_mask])\n",
    "        sec_ss = [th_data.Subset(self, all_idx[sec_mask])\n",
    "                  for sec_mask in sec_masks]\n",
    "\n",
    "        collate_fn = self.get_collate_fn()\n",
    "        if len(train_ss) > 0:\n",
    "            train_dl = th_data.DataLoader(\n",
    "                train_ss,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=pin_memory,\n",
    "                shuffle=True,\n",
    "                drop_last=True  # this is to prevent single data batches that mess with batchnorm\n",
    "            )\n",
    "            train_dl_2 = th_data.DataLoader(\n",
    "                train_ss,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=pin_memory,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "        else:\n",
    "            train_dl = train_dl_2 = None\n",
    "        if len(val_ss) > 0:\n",
    "            val_dl = th_data.DataLoader(\n",
    "                val_ss,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=pin_memory,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "        else:\n",
    "            val_dl = None\n",
    "        if len(test_ss) > 0:\n",
    "            test_dl = th_data.DataLoader(\n",
    "                test_ss,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=pin_memory,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "        else:\n",
    "            test_dl = None\n",
    "        sec_dls = []\n",
    "        for ss in sec_ss:\n",
    "            dl = th_data.DataLoader(\n",
    "                ss,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=pin_memory,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            sec_dls.append(dl)\n",
    "\n",
    "        # set up dl_dict\n",
    "        dl_dict = {}\n",
    "        dl_dict[\"train\"] = train_dl\n",
    "        dl_dict[\"primary\"] = {\n",
    "            \"train\": train_dl_2,\n",
    "            \"val\": val_dl,\n",
    "            \"test\": test_dl\n",
    "        }\n",
    "        dl_dict[\"secondary\"] = {}\n",
    "        for sec_idx, sec_dset in enumerate(self.secondary_dset):\n",
    "            dl_dict[\"secondary\"][f\"{sec_dset}\"] = sec_dls[sec_idx]\n",
    "\n",
    "        # set up split_id_dict\n",
    "        split_id_dict = {}\n",
    "        split_id_dict[\"primary\"] = {}\n",
    "        split_id_dict[\"primary\"][\"train\"] = self.spec_df.iloc[all_idx[train_mask]\n",
    "                                                              ][\"spec_id\"].to_numpy()\n",
    "        split_id_dict[\"primary\"][\"val\"] = self.spec_df.iloc[all_idx[val_mask]\n",
    "                                                            ][\"spec_id\"].to_numpy()\n",
    "        split_id_dict[\"primary\"][\"test\"] = self.spec_df.iloc[all_idx[test_mask]\n",
    "                                                             ][\"spec_id\"].to_numpy()\n",
    "        split_id_dict[\"secondary\"] = {}\n",
    "        for sec_idx, sec_dset in enumerate(self.secondary_dset):\n",
    "            split_id_dict[\"secondary\"][sec_dset] = self.spec_df.iloc[all_idx[sec_masks[sec_idx]]\n",
    "                                                                     ][\"spec_id\"].to_numpy()\n",
    "\n",
    "        return dl_dict, split_id_dict\n",
    "\n",
    "    def get_track_dl(\n",
    "            self,\n",
    "            idx,\n",
    "            num_rand_idx=0,\n",
    "            topk_idx=None,\n",
    "            bottomk_idx=None,\n",
    "            other_idx=None,\n",
    "            spec_ids=None):\n",
    "\n",
    "        track_seed = 5585\n",
    "        track_dl_dict = {}\n",
    "        collate_fn = self.get_collate_fn()\n",
    "        if num_rand_idx > 0:\n",
    "            with utils.np_temp_seed(track_seed):\n",
    "                rand_idx = np.random.choice(\n",
    "                    idx, size=num_rand_idx, replace=False)\n",
    "            rand_dl = th_data.DataLoader(\n",
    "                th_data.Subset(self, rand_idx),\n",
    "                batch_size=1,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=0,\n",
    "                pin_memory=False,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            track_dl_dict[\"rand\"] = rand_dl\n",
    "        if not (topk_idx is None):\n",
    "            topk_idx = idx[topk_idx]\n",
    "            topk_dl = th_data.DataLoader(\n",
    "                th_data.Subset(self, topk_idx),\n",
    "                batch_size=1,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=0,\n",
    "                pin_memory=False,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            track_dl_dict[\"topk\"] = topk_dl\n",
    "        if not (bottomk_idx is None):\n",
    "            bottomk_idx = idx[bottomk_idx]\n",
    "            bottomk_dl = th_data.DataLoader(\n",
    "                th_data.Subset(self, bottomk_idx),\n",
    "                batch_size=1,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=0,\n",
    "                pin_memory=False,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            track_dl_dict[\"bottomk\"] = bottomk_dl\n",
    "        if not (other_idx is None):\n",
    "            other_idx = idx[other_idx]\n",
    "            other_dl = th_data.DataLoader(\n",
    "                th_data.Subset(self, other_idx),\n",
    "                batch_size=1,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=0,\n",
    "                pin_memory=False,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            track_dl_dict[\"other\"] = other_dl\n",
    "        if not (spec_ids is None):\n",
    "            # preserves order\n",
    "            spec_idx = []\n",
    "            for spec_id in spec_ids:\n",
    "                spec_idx.append(\n",
    "                    int(self.spec_df[self.spec_df[\"spec_id\"] == spec_id].index[0]))\n",
    "            spec_idx = np.array(spec_idx)\n",
    "            spec_dl = th_data.DataLoader(\n",
    "                th_data.Subset(self, spec_idx),\n",
    "                batch_size=1,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=0,\n",
    "                pin_memory=False,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            track_dl_dict[\"spec\"] = spec_dl\n",
    "        return track_dl_dict\n",
    "\n",
    " \n",
    "    def get_data_dims(self):\n",
    "\n",
    "        data = self.__getitem__(0)\n",
    "        dim_d = {}\n",
    "        if self.is_fp_dset:\n",
    "            fp_dim = data[\"fp\"].shape[1]\n",
    "        else:\n",
    "            fp_dim = -1\n",
    "        if self.is_graph_dset:\n",
    "            # node\n",
    "            if self.atom_feature_mode == \"pretrain\":\n",
    "                n_dim = -1\n",
    "            else:\n",
    "                n_dim = data[\"graph\"].ndata['h'].shape[1]\n",
    "            # edge\n",
    "            if self.bond_feature_mode == \"none\":\n",
    "                e_dim = 0\n",
    "            elif self.bond_feature_mode == \"pretrain\":\n",
    "                e_dim = -1\n",
    "            else:\n",
    "                e_dim = data[\"graph\"].edata['h'].shape[1]\n",
    "        else:\n",
    "            n_dim = e_dim = -1\n",
    "        c_dim = l_dim = -1\n",
    "        if self.spec_meta_global:\n",
    "            g_dim = data[\"spec_meta\"].shape[1]\n",
    "        else:\n",
    "            g_dim = 0  # -1\n",
    "        o_dim = data[\"spec\"].shape[1]\n",
    "        dim_d = {\n",
    "            \"fp_dim\": fp_dim,\n",
    "            \"n_dim\": n_dim,\n",
    "            \"e_dim\": e_dim,\n",
    "            \"c_dim\": c_dim,\n",
    "            \"l_dim\": l_dim,\n",
    "            \"g_dim\": g_dim,\n",
    "            \"o_dim\": o_dim\n",
    "        }\n",
    "        return dim_d\n",
    "\n",
    "    def get_collate_fn(self):\n",
    "        return MassFormerDataCollator()\n",
    "\n",
    "    def process_entry(self, spec_entry, mol):\n",
    "\n",
    "        # initialize data with shared attributes\n",
    "        spec_feats = self.get_spec_feats(spec_entry)\n",
    "        data_d = {**spec_feats}\n",
    "        data_d[\"smiles\"] = [utils.mol_to_smiles(mol)]\n",
    "        data_d[\"formula\"] = [utils.mol_to_formula(mol)]\n",
    "        graph = utils.mol2graph(mol)\n",
    "        data = utils.graph2data(graph)\n",
    "        items = preprocess_item(data)\n",
    "        data_d.update(items)\n",
    "        return data_d\n",
    "\n",
    "    def load_all(self, keys):\n",
    "\n",
    "        collate_fn = self.get_collate_fn()\n",
    "        dl = th_data.DataLoader(\n",
    "            self,\n",
    "            batch_size=100,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=min(10, len(os.sched_getaffinity(0))),\n",
    "            pin_memory=False,\n",
    "            shuffle=False,\n",
    "            drop_last=False\n",
    "        )\n",
    "        all_ds = []\n",
    "        for b_idx, b in tqdm(enumerate(dl), total=len(dl), desc=\"> load_all\"):\n",
    "            b_d = {}\n",
    "            for k in keys:\n",
    "                b_d[k] = b[k]\n",
    "            all_ds.append(b_d)\n",
    "        all_d = collate_fn(all_ds)\n",
    "        return all_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> mb_na\n",
      "> num_spec = 99, num_mol = 93, num_group = 93, num_ce_per_group = 1.064516129032258\n"
     ]
    }
   ],
   "source": [
    "demo_dataset = BaseDataset(proc_dp=\"data/proc_demo/\", primary_dset=[\"mb_na\"], secondary_dset=[], \n",
    "                           ce_key=\"nce\", inst_type=[\"FT\"], frag_mode=[\"HCD\"], ion_mode=\"P\", process_spec_old=False,\n",
    "                           pos_prec_type=['[M+H]+', '[M+H-H2O]+', '[M+H-2H2O]+', '[M+2H]2+', '[M+H-NH3]+', \"[M+Na]+\"],\n",
    "                           preproc_ce=\"normalize\", mz_max=1000., convert_ce=False, subsample_size=0, num_entries=-1,\n",
    "                           spectrum_normalization=\"l1\", res=[1,2,3,4,5,6,7], mz_bin_res=1., ints_thresh=0., transform=\"log10over3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula', 'edge_attr', 'attn_bias', 'edge_index', 'labels', 'input_nodes', 'input_edges', 'attn_edge_type', 'x', 'num_nodes', 'in_degree', 'y', 'spatial_pos', 'out_degree']\n",
      "gf_collated_keys = ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']\n",
      "Warning: spatial_pos not found in item\n",
      "Warning: labels not found in item\n",
      "Warning: attn_bias not found in item\n",
      "Warning: input_nodes not found in item\n",
      "Warning: attn_edge_type not found in item\n",
      "Warning: in_degree not found in item\n",
      "Warning: input_edges not found in item\n",
      "Warning: out_degree not found in item\n",
      "Warning: spatial_pos not found in item\n",
      "Warning: labels not found in item\n",
      "Warning: attn_bias not found in item\n",
      "Warning: input_nodes not found in item\n",
      "Warning: attn_edge_type not found in item\n",
      "Warning: in_degree not found in item\n",
      "Warning: input_edges not found in item\n",
      "Warning: out_degree not found in item\n",
      "other_collated_keys = ['spec', 'prec_mz', 'prec_mz_bin', 'prec_diff', 'num_peaks', 'inst_type', 'prec_type', 'frag_mode', 'col_energy', 'charge', 'spec_meta', 'mol_id', 'spec_id', 'group_id', 'prec_mz_idx', 'smiles', 'formula']\n",
      "{'attn_bias': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'attn_edge_type': tensor([[[[   0,    0,    0],\n",
      "          [   2,  514, 1026],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0]],\n",
      "\n",
      "         [[   2,  514, 1026],\n",
      "          [   0,    0,    0],\n",
      "          [   2,  514, 1027],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0]],\n",
      "\n",
      "         [[   0,    0,    0],\n",
      "          [   2,  514, 1027],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   5,  514, 1027]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   5,  514, 1027],\n",
      "          [   0,    0,    0]],\n",
      "\n",
      "         [[   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   5,  514, 1027],\n",
      "          [   0,    0,    0],\n",
      "          [   5,  514, 1027]],\n",
      "\n",
      "         [[   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   5,  514, 1027],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   5,  514, 1027],\n",
      "          [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "        [[[   0,    0,    0],\n",
      "          [   2,  514, 1026],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0]],\n",
      "\n",
      "         [[   2,  514, 1026],\n",
      "          [   0,    0,    0],\n",
      "          [   2,  514, 1026],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0]],\n",
      "\n",
      "         [[   0,    0,    0],\n",
      "          [   2,  514, 1026],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0]],\n",
      "\n",
      "         [[   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0]],\n",
      "\n",
      "         [[   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0],\n",
      "          [   0,    0,    0]]]]), 'spatial_pos': tensor([[[1, 2, 3,  ..., 6, 5, 4],\n",
      "         [2, 1, 2,  ..., 5, 4, 3],\n",
      "         [3, 2, 1,  ..., 4, 3, 2],\n",
      "         ...,\n",
      "         [6, 5, 4,  ..., 1, 2, 3],\n",
      "         [5, 4, 3,  ..., 2, 1, 2],\n",
      "         [4, 3, 2,  ..., 3, 2, 1]],\n",
      "\n",
      "        [[1, 2, 3,  ..., 0, 0, 0],\n",
      "         [2, 1, 2,  ..., 0, 0, 0],\n",
      "         [3, 2, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]]), 'in_degree': tensor([[2, 3, 4, 3, 4, 3, 2, 4, 3, 4, 5, 2, 2, 3, 4, 4, 2, 3, 3, 4, 2, 4, 2, 3,\n",
      "         4, 3],\n",
      "        [2, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'input_nodes': tensor([[[4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4]],\n",
      "\n",
      "        [[4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [4],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]]), 'input_edges': tensor([[[[[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   3,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   3,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   3,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   3,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         [[[   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   3,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         [[[   3,  515, 1028],\n",
      "           [   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         [[[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   3,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   3,  515, 1028],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         [[[   6,  515, 1028],\n",
      "           [   3,  515, 1028],\n",
      "           [   3,  515, 1027],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   3,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   6,  515, 1028],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         [[[   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         [[[   3,  515, 1027],\n",
      "           [   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   3,  515, 1027],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         [[[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]],\n",
      "\n",
      "\n",
      "         [[[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]],\n",
      "\n",
      "          [[   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           ...,\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0],\n",
      "           [   0,    0,    0]]]]]), 'out_degree': tensor([[2, 3, 4, 3, 4, 3, 2, 4, 3, 4, 5, 2, 2, 3, 4, 4, 2, 3, 3, 4, 2, 4, 2, 3,\n",
      "         4, 3],\n",
      "        [2, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'labels': tensor([-1., -1.]), 'spec': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'prec_mz': [418.0138, 164.0818], 'prec_mz_bin': [418, 164], 'prec_diff': [9.999999997489795e-05, 0.0004000000000132786], 'num_peaks': [30, 27], 'inst_type': ['FT', 'FT'], 'prec_type': ['[M+H]+', '[M+H]+'], 'frag_mode': ['HCD', 'HCD'], 'col_energy': [30.0, 60.0], 'charge': [1, 1], 'spec_meta': tensor([[-0.8749,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000, -0.8749],\n",
      "        [-0.1061,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000, -0.1061]]), 'mol_id': tensor([9861, 8990]), 'spec_id': tensor([11321, 11425]), 'group_id': tensor([0, 1]), 'prec_mz_idx': tensor([418, 164]), 'smiles': ['COc1cc(OC)n2nc(S(=O)(=O)Nc3c(Cl)ccc(C)c3Cl)nc2n1', 'COCn1nnc2ccccc21'], 'formula': ['C14H13Cl2N5O4S', 'C8H9N3O']}\n"
     ]
    }
   ],
   "source": [
    "collator = demo_dataset.get_collate_fn()\n",
    "# print(demo_dataset[5])\n",
    "# print(demo_dataset[6])\n",
    "collator_example = [demo_dataset[0], demo_dataset[1]]\n",
    "collator_result = collator(collator_example)\n",
    "print(collator_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.graphormer.modeling_graphormer import GraphormerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: graphormer model, MLP from predictor\n",
    "class MFConfig:\n",
    "    def __init__(self):\n",
    "        self.max_nodes=128\n",
    "        self.num_atoms=512 * 9\n",
    "        self.num_in_degree=512\n",
    "        self.num_out_degree=512\n",
    "        self.num_edges=512 * 3\n",
    "        self.num_spatial=512\n",
    "        self.num_edge_dis=128\n",
    "        self.edge_type=\"multi_hop\"\n",
    "        self.multi_hop_max_dist=1024\n",
    "        self.num_encoder_layers=6\n",
    "        self.encoder_embed_dim=512\n",
    "        self.encoder_ffn_embed_dim=512\n",
    "        self.encoder_attention_heads=32\n",
    "        self.dropout=0.0\n",
    "        self.attention_dropout=0.1\n",
    "        self.activation_dropout=0.1\n",
    "        self.encoder_normalize_before=True\n",
    "        self.apply_graphormer_init=True\n",
    "        self.activation_fn=\"gelu\"\n",
    "        self.n_trans_layers_to_freeze=0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_nodes': 128, 'num_atoms': 4608, 'num_in_degree': 512, 'num_out_degree': 512, 'num_edges': 1536, 'num_spatial': 512, 'num_edge_dis': 128, 'edge_type': 'multi_hop', 'multi_hop_max_dist': 1024, 'num_encoder_layers': 6, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 512, 'encoder_attention_heads': 32, 'dropout': 0.0, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_normalize_before': True, 'apply_graphormer_init': True, 'activation_fn': 'gelu', 'n_trans_layers_to_freeze': 0}\n"
     ]
    }
   ],
   "source": [
    "config_example = MFConfig()\n",
    "print(config_example.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parameter config in `GraphormerModel(config)` should be an instance of class `PretrainedConfig`. To create a model from a pretrained model use `model = GraphormerModel.from_pretrained(PRETRAINED_MODEL_NAME)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGraphormerModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_example\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MF/lib/python3.11/site-packages/transformers/models/graphormer/modeling_graphormer.py:784\u001b[0m, in \u001b[0;36mGraphormerModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: GraphormerConfig):\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_nodes \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mmax_nodes\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_encoder \u001b[38;5;241m=\u001b[39m GraphormerGraphEncoder(config)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MF/lib/python3.11/site-packages/transformers/modeling_utils.py:1142\u001b[0m, in \u001b[0;36mPreTrainedModel.__init__\u001b[0;34m(self, config, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m-> 1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter config in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(config)` should be an instance of class \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`PretrainedConfig`. To create a model from a pretrained model use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1146\u001b[0m     )\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;66;03m# Save config and origin of the pretrained weights if given in model\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n",
      "\u001b[0;31mValueError\u001b[0m: Parameter config in `GraphormerModel(config)` should be an instance of class `PretrainedConfig`. To create a model from a pretrained model use `model = GraphormerModel.from_pretrained(PRETRAINED_MODEL_NAME)`"
     ]
    }
   ],
   "source": [
    "model = GraphormerModel(config=config_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MassFormer(nn.Module): # TODO: what should we include here?\n",
    "\n",
    "#     def __init__(self, graphormer_params, mlp_params):\n",
    "\n",
    "#         self.graphormer_module = GraphormerModule(**graphormer_params)\n",
    "#         self.mlp_module = MLPModule(**mlp_params)\n",
    "\n",
    "\n",
    "    \n",
    "#     def get_graph_data(data):\n",
    "#     # TODO: pass the whole dictionary but only accept certain ones\n",
    "\n",
    "#     def get_spec_data(data):\n",
    "#         # \n",
    "\n",
    "#     def forward(self, data):\n",
    "\n",
    "#         graph_data = get_graph_data(data)\n",
    "#         spec_data = get_spec_data(data)\n",
    "#         graph_embedding = self.graphormer_module(graph_data)\n",
    "#         output = self.mlp_module(graph_embedding,spec_data)\n",
    "#         return output\n",
    "        \n",
    "\n",
    "# class MLPModule(nn.Module):\n",
    "\n",
    "#     def __init__(self, **mlp_params):\n",
    "\n",
    "#         ...\n",
    "\n",
    "#     def forward(self, graph_embedding, data):\n",
    "\n",
    "#         ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
